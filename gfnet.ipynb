{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import Error, deepcopy\n",
    "from re import S\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import torch.fft\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import logging\n",
    "import math\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class GlobalFilter(nn.Module):\n",
    "    def __init__(self, dim, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.complex_weight = nn.Parameter(torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02)\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, x, spatial_size=None):\n",
    "        B, N, C = x.shape\n",
    "        if spatial_size is None:\n",
    "            a = b = int(math.sqrt(N))\n",
    "        else:\n",
    "            a, b = spatial_size\n",
    "\n",
    "        x = x.view(B, a, b, C)\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho')\n",
    "        weight = torch.view_as_complex(self.complex_weight)\n",
    "        x = x * weight\n",
    "        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm='ortho')\n",
    "\n",
    "        x = x.reshape(B, N, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = GlobalFilter(dim, h=h, w=w)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(self.filter(self.norm1(x)))))\n",
    "        return x\n",
    "\n",
    "class BlockLayerScale(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU, \n",
    "                norm_layer=nn.LayerNorm, h=14, w=8, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = GlobalFilter(dim, h=h, w=w)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma * self.mlp(self.norm2(self.filter(self.norm1(x)))))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownLayer(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=56, dim_in=64, dim_out=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.proj = nn.Conv2d(dim_in, dim_out, kernel_size=2, stride=2)\n",
    "        self.num_patches = img_size * img_size // 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.size()\n",
    "        x = x.view(B, self.img_size, self.img_size, C).permute(0, 3, 1, 2)\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        x = x.reshape(B, -1, self.dim_out)\n",
    "        return x\n",
    "\n",
    "class GFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 mlp_ratio=4., representation_size=None, uniform_drop=False,\n",
    "                 drop_rate=0., drop_path_rate=0., norm_layer=None, \n",
    "                 dropcls=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            hybrid_backbone (nn.Module): CNN backbone to use in-place of PatchEmbed module\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        h = img_size // patch_size\n",
    "        w = h // 2 + 1\n",
    "\n",
    "        if uniform_drop:\n",
    "            print('using uniform droppath with expect rate', drop_path_rate)\n",
    "            dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        else:\n",
    "            print('using linear droppath with expect rate', drop_path_rate * 0.5)\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, mlp_ratio=mlp_ratio,\n",
    "                drop=drop_rate, drop_path=dpr[i], norm_layer=norm_layer, h=h, w=w)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x).mean(1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.final_dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GFNetPyramid(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=4, num_classes=1000, embed_dim=[64, 128, 256, 512], depth=[2,2,10,4],\n",
    "                 mlp_ratio=[4, 4, 4, 4],\n",
    "                 drop_rate=0., drop_path_rate=0., norm_layer=None, init_values=0.001, no_layerscale=False, dropcls=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            hybrid_backbone (nn.Module): CNN backbone to use in-place of PatchEmbed module\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim[-1]  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = nn.ModuleList()\n",
    "        \n",
    "        patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim[0])\n",
    "        num_patches = patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim[0]))\n",
    "\n",
    "        self.patch_embed.append(patch_embed)\n",
    "\n",
    "        sizes = [56, 28, 14, 7]\n",
    "        for i in range(4):\n",
    "            sizes[i] = sizes[i] * img_size // 224\n",
    "\n",
    "        for i in range(3):\n",
    "            patch_embed = DownLayer(sizes[i], embed_dim[i], embed_dim[i+1])\n",
    "            num_patches = patch_embed.num_patches\n",
    "            self.patch_embed.append(patch_embed)\n",
    "\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            h = sizes[i]\n",
    "            w = h // 2 + 1\n",
    "\n",
    "            if no_layerscale:\n",
    "                print('using standard block')\n",
    "                blk = nn.Sequential(*[\n",
    "                    Block(\n",
    "                    dim=embed_dim[i], mlp_ratio=mlp_ratio[i],\n",
    "                    drop=drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer, h=h, w=w)\n",
    "                for j in range(depth[i])\n",
    "                ])\n",
    "            else:\n",
    "                print('using layerscale block')\n",
    "                blk = nn.Sequential(*[\n",
    "                    BlockLayerScale(\n",
    "                    dim=embed_dim[i], mlp_ratio=mlp_ratio[i],\n",
    "                    drop=drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer, h=h, w=w, init_values=init_values)\n",
    "                for j in range(depth[i])\n",
    "                ])\n",
    "            self.blocks.append(blk)\n",
    "            cur += depth[i]\n",
    "\n",
    "        # Classifier head\n",
    "        self.norm = norm_layer(embed_dim[-1])\n",
    "\n",
    "        self.head = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.patch_embed[i](x)\n",
    "            if i == 0:\n",
    "                x = x + self.pos_embed\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        x = self.norm(x).mean(1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.final_dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if True:\n",
    "        posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "        ntok_new -= 1\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(math.sqrt(ntok_new))\n",
    "    _logger.info('Position embedding grid-size from %s to %s', gs_old, gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_new, gs_new), mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new * gs_new, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(v, model.pos_embed)\n",
    "        out_dict[k] = v\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\cocom\\AppData\\Local\\Temp\\ipykernel_21004\\3368972940.py:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  dataset = datasets.ImageFolder(root='ADNI_AD_NC_2D\\AD_NC', transform=transform)\n",
      "INFO:__main__:Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using linear droppath with expect rate 0.0\n"
     ]
    }
   ],
   "source": [
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load ADNI dataset\n",
    "dataset = datasets.ImageFolder(root='ADNI_AD_NC_2D\\AD_NC', transform=transform)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "model = GFNet(img_size=224, patch_size=16, in_chans=3, num_classes=2, embed_dim=768, depth=12, mlp_ratio=4, drop_rate=0, drop_path_rate=0.)\n",
    "model.head = nn.Linear(model.num_features, 2)  # Assuming binary classification\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        _logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        _logger.info(f'Validation Accuracy: {correct/total}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    test_accuracy = correct / total\n",
    "    _logger.info(f'Test Accuracy: {test_accuracy}')\n",
    "    return test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "from copy import Error, deepcopy\n",
    "from re import S\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "import torch.fft\n",
    "from torch.nn.modules.container import Sequential\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import logging\n",
    "import math\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def _cfg(url='', **kwargs):\n",
    "    return {\n",
    "        'url': url,\n",
    "        'num_classes': 1000, 'input_size': (3, 224, 224), 'pool_size': None,\n",
    "        'crop_pct': .9, 'interpolation': 'bicubic',\n",
    "        'mean': IMAGENET_DEFAULT_MEAN, 'std': IMAGENET_DEFAULT_STD,\n",
    "        'first_conv': 'patch_embed.proj', 'classifier': 'head',\n",
    "        **kwargs\n",
    "    }\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class GlobalFilter(nn.Module):\n",
    "    def __init__(self, dim, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.complex_weight = nn.Parameter(torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02)\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, x, spatial_size=None):\n",
    "        B, N, C = x.shape\n",
    "        if spatial_size is None:\n",
    "            a = b = int(math.sqrt(N))\n",
    "        else:\n",
    "            a, b = spatial_size\n",
    "\n",
    "        x = x.view(B, a, b, C)\n",
    "\n",
    "        x = x.to(torch.float32)\n",
    "\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho')\n",
    "        weight = torch.view_as_complex(self.complex_weight)\n",
    "        x = x * weight\n",
    "        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm='ortho')\n",
    "\n",
    "        x = x.reshape(B, N, C)\n",
    "\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = GlobalFilter(dim, h=h, w=w)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(self.filter(self.norm1(x)))))\n",
    "        return x\n",
    "\n",
    "class BlockLayerScale(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, mlp_ratio=4., drop=0., drop_path=0., act_layer=nn.GELU, \n",
    "                norm_layer=nn.LayerNorm, h=14, w=8, init_values=1e-5):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.filter = GlobalFilter(dim, h=h, w=w)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.drop_path(self.gamma * self.mlp(self.norm2(self.filter(self.norm1(x)))))\n",
    "        return x\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        # FIXME look at relaxing size constraints\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownLayer(nn.Module):\n",
    "    \"\"\" Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=56, dim_in=64, dim_out=128):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.dim_in = dim_in\n",
    "        self.dim_out = dim_out\n",
    "        self.proj = nn.Conv2d(dim_in, dim_out, kernel_size=2, stride=2)\n",
    "        self.num_patches = img_size * img_size // 4\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.size()\n",
    "        x = x.view(B, self.img_size, self.img_size, C).permute(0, 3, 1, 2)\n",
    "        x = self.proj(x).permute(0, 2, 3, 1)\n",
    "        x = x.reshape(B, -1, self.dim_out)\n",
    "        return x\n",
    "\n",
    "class GFNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 mlp_ratio=4., representation_size=None, uniform_drop=False,\n",
    "                 drop_rate=0., drop_path_rate=0., norm_layer=None, \n",
    "                 dropcls=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            hybrid_backbone (nn.Module): CNN backbone to use in-place of PatchEmbed module\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        h = img_size // patch_size\n",
    "        w = h // 2 + 1\n",
    "\n",
    "        if uniform_drop:\n",
    "            print('using uniform droppath with expect rate', drop_path_rate)\n",
    "            dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        else:\n",
    "            print('using linear droppath with expect rate', drop_path_rate * 0.5)\n",
    "            dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        # dpr = [drop_path_rate for _ in range(depth)]  # stochastic depth decay rule\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, mlp_ratio=mlp_ratio,\n",
    "                drop=drop_rate, drop_path=dpr[i], norm_layer=norm_layer, h=h, w=w)\n",
    "            for i in range(depth)])\n",
    "        \n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size:\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                ('fc', nn.Linear(embed_dim, representation_size)),\n",
    "                ('act', nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "\n",
    "        x = self.norm(x).mean(1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.final_dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class GFNetPyramid(nn.Module):\n",
    "    \n",
    "    def __init__(self, img_size=224, patch_size=4, num_classes=1000, embed_dim=[64, 128, 256, 512], depth=[2,2,10,4],\n",
    "                 mlp_ratio=[4, 4, 4, 4],\n",
    "                 drop_rate=0., drop_path_rate=0., norm_layer=None, init_values=0.001, no_layerscale=False, dropcls=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            hybrid_backbone (nn.Module): CNN backbone to use in-place of PatchEmbed module\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim[-1]  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = nn.ModuleList()\n",
    "        \n",
    "        patch_embed = PatchEmbed(\n",
    "                img_size=img_size, patch_size=patch_size, in_chans=3, embed_dim=embed_dim[0])\n",
    "        num_patches = patch_embed.num_patches\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, embed_dim[0]))\n",
    "\n",
    "        self.patch_embed.append(patch_embed)\n",
    "\n",
    "        sizes = [56, 28, 14, 7]\n",
    "        for i in range(4):\n",
    "            sizes[i] = sizes[i] * img_size // 224\n",
    "\n",
    "        for i in range(3):\n",
    "            patch_embed = DownLayer(sizes[i], embed_dim[i], embed_dim[i+1])\n",
    "            num_patches = patch_embed.num_patches\n",
    "            self.patch_embed.append(patch_embed)\n",
    "\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depth))]  # stochastic depth decay rule\n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            h = sizes[i]\n",
    "            w = h // 2 + 1\n",
    "\n",
    "            if no_layerscale:\n",
    "                print('using standard block')\n",
    "                blk = nn.Sequential(*[\n",
    "                    Block(\n",
    "                    dim=embed_dim[i], mlp_ratio=mlp_ratio[i],\n",
    "                    drop=drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer, h=h, w=w)\n",
    "                for j in range(depth[i])\n",
    "                ])\n",
    "            else:\n",
    "                print('using layerscale block')\n",
    "                blk = nn.Sequential(*[\n",
    "                    BlockLayerScale(\n",
    "                    dim=embed_dim[i], mlp_ratio=mlp_ratio[i],\n",
    "                    drop=drop_rate, drop_path=dpr[cur + j], norm_layer=norm_layer, h=h, w=w, init_values=init_values)\n",
    "                for j in range(depth[i])\n",
    "                ])\n",
    "            self.blocks.append(blk)\n",
    "            cur += depth[i]\n",
    "\n",
    "        # Classifier head\n",
    "        self.norm = norm_layer(embed_dim[-1])\n",
    "\n",
    "        self.head = nn.Linear(self.num_features, num_classes)\n",
    "\n",
    "        if dropcls > 0:\n",
    "            print('dropout %.2f before classifier' % dropcls)\n",
    "            self.final_dropout = nn.Dropout(p=dropcls)\n",
    "        else:\n",
    "            self.final_dropout = nn.Identity()\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_classifier(self):\n",
    "        return self.head\n",
    "\n",
    "    def reset_classifier(self, num_classes, global_pool=''):\n",
    "        self.num_classes = num_classes\n",
    "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.patch_embed[i](x)\n",
    "            if i == 0:\n",
    "                x = x + self.pos_embed\n",
    "            x = self.blocks[i](x)\n",
    "\n",
    "        x = self.norm(x).mean(1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.final_dropout(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "def resize_pos_embed(posemb, posemb_new):\n",
    "    # Rescale the grid of position embeddings when loading from state_dict. Adapted from\n",
    "    # https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224\n",
    "    _logger.info('Resized position embedding: %s to %s', posemb.shape, posemb_new.shape)\n",
    "    ntok_new = posemb_new.shape[1]\n",
    "    if True:\n",
    "        posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
    "        ntok_new -= 1\n",
    "    else:\n",
    "        posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
    "    gs_old = int(math.sqrt(len(posemb_grid)))\n",
    "    gs_new = int(math.sqrt(ntok_new))\n",
    "    _logger.info('Position embedding grid-size from %s to %s', gs_old, gs_new)\n",
    "    posemb_grid = posemb_grid.reshape(1, gs_old, gs_old, -1).permute(0, 3, 1, 2)\n",
    "    posemb_grid = F.interpolate(posemb_grid, size=(gs_new, gs_new), mode='bilinear')\n",
    "    posemb_grid = posemb_grid.permute(0, 2, 3, 1).reshape(1, gs_new * gs_new, -1)\n",
    "    posemb = torch.cat([posemb_tok, posemb_grid], dim=1)\n",
    "    return posemb\n",
    "\n",
    "\n",
    "def checkpoint_filter_fn(state_dict, model):\n",
    "    \"\"\" convert patch embedding weight from manual patchify + linear proj to conv\"\"\"\n",
    "    out_dict = {}\n",
    "    if 'model' in state_dict:\n",
    "        # For deit models\n",
    "        state_dict = state_dict['model']\n",
    "    for k, v in state_dict.items():\n",
    "        if 'patch_embed.proj.weight' in k and len(v.shape) < 4:\n",
    "            # For old models that I trained prior to conv based patchification\n",
    "            O, I, H, W = model.patch_embed.proj.weight.shape\n",
    "            v = v.reshape(O, -1, H, W)\n",
    "        elif k == 'pos_embed' and v.shape != model.pos_embed.shape:\n",
    "            # To resize pos embedding when using model at different size from pretrained weights\n",
    "            v = resize_pos_embed(v, model.pos_embed)\n",
    "        out_dict[k] = v\n",
    "    return out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\cocom\\AppData\\Local\\Temp\\ipykernel_21004\\3484060566.py:19: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  dataset = datasets.ImageFolder(root='ADNI_AD_NC_2D\\AD_NC', transform=transform)\n",
      "INFO:__main__:Using device: cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using linear droppath with expect rate 0.0\n"
     ]
    }
   ],
   "source": [
    "# Assuming GFNet and other necessary classes are defined as provided\n",
    "\n",
    "# Initialize logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "_logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Data Preparation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load ADNI dataset\n",
    "dataset = datasets.ImageFolder(root='ADNI_AD_NC_2D\\AD_NC', transform=transform)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Model Initialization\n",
    "model = GFNet(img_size=224, patch_size=16, in_chans=3, num_classes=2, embed_dim=768, depth=12, mlp_ratio=4, drop_rate=0, drop_path_rate=0.)\n",
    "model.head = nn.Linear(model.num_features, 2)  # Assuming binary classification\n",
    "model.to(device)  # Move model to GPU\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        _logger.info(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        _logger.info(f'Validation Accuracy: {correct/total}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    _logger.info(f'Confusion Matrix:\\n{cm}')\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'AD'], yticklabels=['Normal', 'AD'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute accuracy\n",
    "    correct = sum(np.array(all_preds) == np.array(all_labels))\n",
    "    total = len(all_labels)\n",
    "    test_accuracy = correct / total\n",
    "    _logger.info(f'Test Accuracy: {test_accuracy}')\n",
    "    return test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Epoch 1/25, Loss: 0.6466486288937267\n",
      "INFO:__main__:Validation Accuracy: 0.6998689384010485\n",
      "INFO:__main__:Epoch 2/25, Loss: 0.6122082846607276\n",
      "INFO:__main__:Validation Accuracy: 0.6998689384010485\n",
      "INFO:__main__:Epoch 3/25, Loss: 0.6020635841135493\n",
      "INFO:__main__:Validation Accuracy: 0.6998689384010485\n",
      "INFO:__main__:Epoch 4/25, Loss: 0.5856585387073591\n",
      "INFO:__main__:Validation Accuracy: 0.7066404543468764\n",
      "INFO:__main__:Epoch 5/25, Loss: 0.5352282126149731\n",
      "INFO:__main__:Validation Accuracy: 0.7667103538663171\n",
      "INFO:__main__:Epoch 6/25, Loss: 0.39861706988941764\n",
      "INFO:__main__:Validation Accuracy: 0.8422892092616864\n",
      "INFO:__main__:Epoch 7/25, Loss: 0.22665207289873127\n",
      "INFO:__main__:Validation Accuracy: 0.8938401048492791\n",
      "INFO:__main__:Epoch 8/25, Loss: 0.11452570379276698\n",
      "INFO:__main__:Validation Accuracy: 0.8778942769768457\n",
      "INFO:__main__:Epoch 9/25, Loss: 0.07309021707833327\n",
      "INFO:__main__:Validation Accuracy: 0.9242027086063783\n",
      "INFO:__main__:Epoch 10/25, Loss: 0.04885164971033273\n",
      "INFO:__main__:Validation Accuracy: 0.927916120576671\n",
      "INFO:__main__:Epoch 11/25, Loss: 0.04764976895004399\n",
      "INFO:__main__:Validation Accuracy: 0.9305373525557011\n",
      "INFO:__main__:Epoch 12/25, Loss: 0.0329567190483812\n",
      "INFO:__main__:Validation Accuracy: 0.91044124071647\n",
      "INFO:__main__:Epoch 13/25, Loss: 0.03613359598152048\n",
      "INFO:__main__:Validation Accuracy: 0.9333770205329839\n",
      "INFO:__main__:Epoch 14/25, Loss: 0.036273045690687\n",
      "INFO:__main__:Validation Accuracy: 0.9309742245522062\n",
      "INFO:__main__:Epoch 15/25, Loss: 0.025403968061402046\n",
      "INFO:__main__:Validation Accuracy: 0.935124508519004\n",
      "INFO:__main__:Epoch 16/25, Loss: 0.025045901301627475\n",
      "INFO:__main__:Validation Accuracy: 0.9335954565312363\n",
      "INFO:__main__:Epoch 17/25, Loss: 0.025651204423476754\n",
      "INFO:__main__:Validation Accuracy: 0.927916120576671\n",
      "INFO:__main__:Epoch 18/25, Loss: 0.026983183207116537\n",
      "INFO:__main__:Validation Accuracy: 0.938182612494539\n",
      "INFO:__main__:Epoch 19/25, Loss: 0.024737543225167097\n",
      "INFO:__main__:Validation Accuracy: 0.9512887723896898\n",
      "INFO:__main__:Epoch 20/25, Loss: 0.01991328805407337\n",
      "INFO:__main__:Validation Accuracy: 0.9436435124508519\n",
      "INFO:__main__:Epoch 21/25, Loss: 0.02024220404685306\n",
      "INFO:__main__:Validation Accuracy: 0.9504150283966798\n",
      "INFO:__main__:Epoch 22/25, Loss: 0.02565389082495724\n",
      "INFO:__main__:Validation Accuracy: 0.9578418523372652\n",
      "INFO:__main__:Epoch 23/25, Loss: 0.02012385101996536\n",
      "INFO:__main__:Validation Accuracy: 0.9434250764525994\n",
      "INFO:__main__:Epoch 24/25, Loss: 0.01649081781858137\n",
      "INFO:__main__:Validation Accuracy: 0.9530362603757099\n",
      "INFO:__main__:Epoch 25/25, Loss: 0.01666320605048797\n",
      "INFO:__main__:Validation Accuracy: 0.9388379204892966\n",
      "INFO:__main__:Confusion Matrix:\n",
      "[[1243  101]\n",
      " [ 241 2993]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAAJuCAYAAAA3hHQxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABM0ElEQVR4nO3de3zP9f//8fvbzNs282Zmm5XzKWxFFKOcc4iQCikRTZ9I7YMO6lOUMnwKIYeEOZV8Cp20IofycWxZzqIc8mnLoZnMbDOv3x/9vL+vt5FNT3tv3K7fy+vytdfr+Xq9Hu/35/Lx8dj9+Xy9HJZlWQIAAAAAg4p4uwAAAAAA1x4aDQAAAADG0WgAAAAAMI5GAwAAAIBxNBoAAAAAjKPRAAAAAGAcjQYAAAAA42g0AAAAABhHowEAAADAOBoNAAXW1q1b9eijj6py5coqXry4SpQooVtvvVVjx47V77//flXvvWXLFjVr1kwul0sOh0MTJkwwfg+Hw6ERI0YYv+7lxMXFyeFwyOFwaPXq1TmOW5alatWqyeFwqHnz5ld0jylTpiguLi5P56xevfqSNQEACp+i3i4AAC5mxowZGjBggGrWrKlnnnlGtWvXVlZWlr777jtNmzZN69ev15IlS67a/fv27au0tDQtXLhQpUuXVqVKlYzfY/369brxxhuNXze3AgMDNXPmzBzNxJo1a/TTTz8pMDDwiq89ZcoUBQcHq0+fPrk+59Zbb9X69etVu3btK74vAKDgoNEAUOCsX79eTzzxhO666y4tXbpUTqfTfeyuu+7SkCFDFB8ff1Vr2L59u6Kjo9W+ffurdo9GjRpdtWvnRvfu3bVgwQK9/fbbKlmypHv/zJkzFRUVpZMnT+ZLHVlZWXI4HCpZsqTXvxMAgDlMnQJQ4IwaNUoOh0PvvPOOR5NxXrFixdSpUyf3z+fOndPYsWN10003yel0KiQkRI888ogOHz7scV7z5s0VERGhzZs3684775S/v7+qVKmi0aNH69y5c5L+b1rR2bNnNXXqVPcUI0kaMWKE+8925885cOCAe9/KlSvVvHlzlSlTRn5+fqpQoYLuu+8+nT592j3mYlOntm/frs6dO6t06dIqXry46tatqzlz5niMOT/F6P3339eLL76o8PBwlSxZUq1bt9aePXty9yVLevDBByVJ77//vntfamqqPvroI/Xt2/ei57zyyitq2LChgoKCVLJkSd16662aOXOmLMtyj6lUqZJ27NihNWvWuL+/84nQ+drnzZunIUOG6IYbbpDT6dS+fftyTJ06duyYypcvr8aNGysrK8t9/Z07dyogIEC9evXK9WcFAOQ/Gg0ABUp2drZWrlyp+vXrq3z58rk654knntBzzz2nu+66S5988olGjhyp+Ph4NW7cWMeOHfMYm5ycrIceekgPP/ywPvnkE7Vv317Dhg3T/PnzJUkdOnTQ+vXrJUn333+/1q9f7/45tw4cOKAOHTqoWLFimjVrluLj4zV69GgFBAQoMzPzkuft2bNHjRs31o4dOzRx4kQtXrxYtWvXVp8+fTR27Ngc41944QUdPHhQ7777rt555x3t3btX99xzj7Kzs3NVZ8mSJXX//fdr1qxZ7n3vv/++ihQpou7du1/ysz3++ONatGiRFi9erK5du2rQoEEaOXKke8ySJUtUpUoV1atXz/39XTjNbdiwYTp06JCmTZumTz/9VCEhITnuFRwcrIULF2rz5s167rnnJEmnT5/WAw88oAoVKmjatGm5+pwAAC+xAKAASU5OtiRZPXr0yNX4Xbt2WZKsAQMGeOzfuHGjJcl64YUX3PuaNWtmSbI2btzoMbZ27dpW27ZtPfZJsgYOHOixb/jw4dbF/tqcPXu2Jcnav3+/ZVmW9eGHH1qSrMTExL+sXZI1fPhw9889evSwnE6ndejQIY9x7du3t/z9/a0TJ05YlmVZq1atsiRZd999t8e4RYsWWZKs9evX/+V9z9e7efNm97W2b99uWZZl3XbbbVafPn0sy7KsOnXqWM2aNbvkdbKzs62srCzr1VdftcqUKWOdO3fOfexS556/X9OmTS95bNWqVR77x4wZY0mylixZYvXu3dvy8/Oztm7d+pefEQDgfSQaAAq1VatWSVKORce33367atWqpa+//tpjf1hYmG6//XaPfTfffLMOHjxorKa6deuqWLFi6t+/v+bMmaOff/45V+etXLlSrVq1ypHk9OnTR6dPn86RrNinj0l/fg5JefoszZo1U9WqVTVr1ixt27ZNmzdvvuS0qfM1tm7dWi6XSz4+PvL19dXLL7+s48eP68iRI7m+73333Zfrsc8884w6dOigBx98UHPmzNGkSZMUGRmZ6/MBAN5BowGgQAkODpa/v7/279+fq/HHjx+XJJUrVy7HsfDwcPfx88qUKZNjnNPpVHp6+hVUe3FVq1bVihUrFBISooEDB6pq1aqqWrWq3nrrrb887/jx45f8HOeP2134Wc6vZ8nLZ3E4HHr00Uc1f/58TZs2TTVq1NCdd9550bGbNm1SmzZtJP35VLD//ve/2rx5s1588cU83/din/OvauzTp4/OnDmjsLAw1mYAQCFBowGgQPHx8VGrVq2UkJCQYzH3xZz/x3ZSUlKOY7/++quCg4ON1Va8eHFJUkZGhsf+C9eBSNKdd96pTz/9VKmpqdqwYYOioqIUExOjhQsXXvL6ZcqUueTnkGT0s9j16dNHx44d07Rp0/Too49ectzChQvl6+urzz77TN26dVPjxo3VoEGDK7rnxRbVX0pSUpIGDhyounXr6vjx4xo6dOgV3RMAkL9oNAAUOMOGDZNlWYqOjr7o4umsrCx9+umnkqSWLVtKknsx93mbN2/Wrl271KpVK2N1nX9y0tatWz32n6/lYnx8fNSwYUO9/fbbkqTvv//+kmNbtWqllStXuhuL8+bOnSt/f/+r9ujXG264Qc8884zuuece9e7d+5LjHA6HihYtKh8fH/e+9PR0zZs3L8dYUylRdna2HnzwQTkcDn3xxReKjY3VpEmTtHjx4r99bQDA1cV7NAAUOFFRUZo6daoGDBig+vXr64knnlCdOnWUlZWlLVu26J133lFERITuuece1axZU/3799ekSZNUpEgRtW/fXgcOHNBLL72k8uXL65///Kexuu6++24FBQWpX79+evXVV1W0aFHFxcXpl19+8Rg3bdo0rVy5Uh06dFCFChV05swZ95OdWrdufcnrDx8+XJ999platGihl19+WUFBQVqwYIE+//xzjR07Vi6Xy9hnudDo0aMvO6ZDhw4aN26cevbsqf79++v48eN64403LvoI4sjISC1cuFAffPCBqlSpouLFi1/Ruorhw4fr22+/1VdffaWwsDANGTJEa9asUb9+/VSvXj1Vrlw5z9cEAOQPGg0ABVJ0dLRuv/12jR8/XmPGjFFycrJ8fX1Vo0YN9ezZU08++aR77NSpU1W1alXNnDlTb7/9tlwul9q1a6fY2NiLrsm4UiVLllR8fLxiYmL08MMPq1SpUnrsscfUvn17PfbYY+5xdevW1VdffaXhw4crOTlZJUqUUEREhD755BP3GoeLqVmzptatW6cXXnhBAwcOVHp6umrVqqXZs2fn6Q3bV0vLli01a9YsjRkzRvfcc49uuOEGRUdHKyQkRP369fMY+8orrygpKUnR0dH6448/VLFiRY/3jOTG8uXLFRsbq5deeskjmYqLi1O9evXUvXt3rV27VsWKFTPx8QAAhjksy/aWJQAAAAAwgDUaAAAAAIyj0QAAAABgHI0GAAAAAONoNAAAAAAYR6MBAAAAwDgaDQAAAADG0WgAAAAAMO6afGHflHUHvF0CABjV9/ZK3i4BAIwqXoD/FepX78nLDzIkfcvkfLtXfiPRAAAAAGBcAe4lAQAAAC9w8Lt4E/gWAQAAABhHogEAAADYORzeruCaQKIBAAAAwDgSDQAAAMCONRpG8C0CAAAAMI5EAwAAALBjjYYRJBoAAAAAjCPRAAAAAOxYo2EE3yIAAAAA40g0AAAAADvWaBhBogEAAADAOBINAAAAwI41GkbwLQIAAAAwjkYDAAAAgHFMnQIAAADsWAxuBIkGAAAAAONINAAAAAA7FoMbwbcIAAAAwDgSDQAAAMCONRpGkGgAAAAAMI5EAwAAALBjjYYRfIsAAAAAjCPRAAAAAOxYo2EEiQYAAAAA40g0AAAAADvWaBjBtwgAAADAOBINAAAAwI5Ewwi+RQAAAADGkWgAAAAAdkV46pQJJBoAAAAAjCPRAAAAAOxYo2EE3yIAAAAA42g0AAAAABjH1CkAAADAzsFicBNINAAAAAAYR6IBAAAA2LEY3Ai+RQAAAADGkWgAAAAAdqzRMIJEAwAAAIBxJBoAAACAHWs0jOBbBAAAAGAciQYAAABgxxoNI0g0AAAAABhHogEAAADYsUbDCL5FAAAAAMaRaAAAAAB2rNEwgkQDAAAAgHEkGgAAAIAdazSM4FsEAAAAYByJBgAAAGDHGg0jSDQAAAAAGEeiAQAAANixRsMIvkUAAAAAxtFoAAAAADCOqVMAAACAHVOnjOBbBAAAAGAciQYAAABgx+NtjSDRAAAAAGAciQYAAABgxxoNI/gWAQAAABhHogEAAADYsUbDCBINAAAAAMaRaAAAAAB2rNEwgm8RAAAAgHEkGgAAAIAdazSMINEAAAAAYByJBgAAAGDjINEwgkQDAAAAgHEkGgAAAIANiYYZJBoAAAAAjCPRAAAAAOwINIwg0QAAAABgHI0GAAAAAOOYOgUAAADYsBjcDBINAAAAAMaRaAAAAAA2JBpmkGgAAAAAMI5EAwAAALAh0TCDRAMAAACAcSQaAAAAgA2JhhkkGgAAAACMI9EAAAAA7Ag0jCDRAAAAAGAciQYAAABgwxoNM0g0AAAAABhHowEAAADYOByOfNvyIjY2VrfddpsCAwMVEhKiLl26aM+ePR5j+vTpk+MejRo18hiTkZGhQYMGKTg4WAEBAerUqZMOHz7sMSYlJUW9evWSy+WSy+VSr169dOLEiTzVS6MBAAAAFAJr1qzRwIEDtWHDBi1fvlxnz55VmzZtlJaW5jGuXbt2SkpKcm/Lli3zOB4TE6MlS5Zo4cKFWrt2rU6dOqWOHTsqOzvbPaZnz55KTExUfHy84uPjlZiYqF69euWpXtZoAAAAADYFdY1GfHy8x8+zZ89WSEiIEhIS1LRpU/d+p9OpsLCwi14jNTVVM2fO1Lx589S6dWtJ0vz581W+fHmtWLFCbdu21a5duxQfH68NGzaoYcOGkqQZM2YoKipKe/bsUc2aNXNVL4kGAAAA4CUZGRk6efKkx5aRkZGrc1NTUyVJQUFBHvtXr16tkJAQ1ahRQ9HR0Tpy5Ij7WEJCgrKystSmTRv3vvDwcEVERGjdunWSpPXr18vlcrmbDElq1KiRXC6Xe0xu0GgAAAAANvm5RiM2Nta9DuL8Fhsbe9kaLcvS4MGDdccddygiIsK9v3379lqwYIFWrlypN998U5s3b1bLli3dzUtycrKKFSum0qVLe1wvNDRUycnJ7jEhISE57hkSEuIekxtMnQIAAAC8ZNiwYRo8eLDHPqfTednznnzySW3dulVr16712N+9e3f3nyMiItSgQQNVrFhRn3/+ubp27XrJ61mW5TFl7GLTxy4cczk0GgAAAIBdPi7RcDqduWos7AYNGqRPPvlE33zzjW688ca/HFuuXDlVrFhRe/fulSSFhYUpMzNTKSkpHqnGkSNH1LhxY/eY3377Lce1jh49qtDQ0FzXydQpAAAAoBCwLEtPPvmkFi9erJUrV6py5cqXPef48eP65ZdfVK5cOUlS/fr15evrq+XLl7vHJCUlafv27e5GIyoqSqmpqdq0aZN7zMaNG5WamuoekxskGgAAAEAhMHDgQL333nv6+OOPFRgY6F4v4XK55Ofnp1OnTmnEiBG67777VK5cOR04cEAvvPCCgoODde+997rH9uvXT0OGDFGZMmUUFBSkoUOHKjIy0v0Uqlq1aqldu3aKjo7W9OnTJUn9+/dXx44dc/3EKYlGAwAAAPBQUB9vO3XqVElS8+bNPfbPnj1bffr0kY+Pj7Zt26a5c+fqxIkTKleunFq0aKEPPvhAgYGB7vHjx49X0aJF1a1bN6Wnp6tVq1aKi4uTj4+Pe8yCBQv01FNPuZ9O1alTJ02ePDlP9Tosy7Ku8LMWWFPWHfB2CQBgVN/bK3m7BAAwqngB/nV3cJ+F+XavY3E98u1e+a0A/0cMAAAA5L+CmmgUNiwGBwAAAGAciQYAAABgQ6JhBokGAAAAAONINAAAAAA7Ag0jSDQAAAAAGEeiAQAAANiwRsMMEg0AAAAAxpFoAAAAADYkGmaQaAAAAAAwjkQDAAAAsCHRMINEAwAAAIBxJBoAAACADYmGGSQaAAAAAIzzWqJx8uTJXI8tWbLkVawEAAAAsCHQMMJrjUapUqUuG0tZliWHw6Hs7Ox8qgoAAACACV5rNFatWuWtWwMAAAC4yrzWaDRr1sxbtwYAAAAuicXgZhSop06dPn1ahw4dUmZmpsf+m2++2UsVAQAAALgSBaLROHr0qB599FF98cUXFz3OGg0AAADkFxINMwrE421jYmKUkpKiDRs2yM/PT/Hx8ZozZ46qV6+uTz75xNvlAQAAAMijApForFy5Uh9//LFuu+02FSlSRBUrVtRdd92lkiVLKjY2Vh06dPB2iQAAALhOkGiYUSASjbS0NIWEhEiSgoKCdPToUUlSZGSkvv/+e2+WBgAAAOAKFIhGo2bNmtqzZ48kqW7dupo+fbr+97//adq0aSpXrpyXqwMAAMB1xZGP2zWsQEydiomJUVJSkiRp+PDhatu2rRYsWKBixYopLi7Ou8UBAAAAyLMC0Wg89NBD7j/Xq1dPBw4c0O7du1WhQgUFBwd7sTIAAABcb1ijYUaBaDQu5O/vr1tvvdXbZQAAAAC4QgWi0bAsSx9++KFWrVqlI0eO6Ny5cx7HFy9e7KXKAAAAcL0h0TCjQDQaTz/9tN555x21aNFCoaGh/IcLAAAAFHIFotGYP3++Fi9erLvvvtvbpQAAAOA6xy+9zSgQjYbL5VKVKlW8XQauQ//bs00JX/xHRw7uVdqJ39Vx0HBVvbWxJCn77FmtXxynA1s3K/Vokpz+ASpfu56a3N9PJUqXyXEty7L08fh/6eC27zyuI0mfvDVcRw/9pPSTJ+QMCFSF2vXU5IGLXwcATEv4brPiZs3Urp3bdfToUY2f+LZatmrtPm5ZlqZNmayP/vOBTp48qcibb9Gwf72satWqu8d8uOgDfbHsM+3auUNpaWn6dv1mlSxZ0hsfB0AhUSDeozFixAi98sorSk9P93YpuM5kZZxRcPkqav7QwBzHzmZm6MjBfbq9U0/1HPG2Ojz5sk4k/0+fThx+0Wtt+WqJLvVA7BtvukV3D3hRj8TOVIeB/1LqkV+17O2RJj8KAFxSevpp1axZU8+/+PJFj8+eOUPz5szW8y++rAUffKgywcH6x2OPKi3tlHvMmTPpatzkTvWL/kd+lQ14jcPhyLftWlYgEo0HHnhA77//vkJCQlSpUiX5+vp6HOft4LhaKt18myrdfNtFjzn9A9T1mdEe+5o9NEAfjHxKJ48fUckyIe79Rw/9pC1ffqQewyfp3ZgHc1zr1rZd3X8uGRyqBh2669NJryj77Fn5FC0Q/zUEcA27485muuPOZhc9ZlmWFsybq8f6/0Ot72ojSXpt1Bi1bNpYyz7/TA906yFJeviRPpKkzZs25kvNAAq/AvEvnD59+ighIUEPP/wwi8FRoGWmp0kOh5z+Ae59WRlnFD99tJo/PFABrqDLXuPMqZPavX6lylWrTZMBwOv+d/iwjh07qqgmd7j3FStWTPUb3KYftmxxNxrAdYV/ihpRIP6V8/nnn+vLL7/UHXfccfnBF8jIyFBGRobHvqzMDPkWc5oqD5Aknc3K1H8/nKWaDVvI6fd/jcY3709Xuaq1PdZkXMzaRe/qh68/0dnMDIVVraVOMa9e7ZIB4LKOHTsqSSpTxnPNWJkywfr111+9URKAa0SBWKNRvnz5K15QFhsbK5fL5bF9NW+q4Qpxvcs+e1ZfTB0l65ylFo886d7/85b1+mVXopr2vPyc5frtH1DPV6aoy9BRchQpoq9m/FuWZV3NsgEg1y6cTWBZlphggOsVazTMKBCJxptvvqlnn31W06ZNU6VKlfJ07rBhwzR48GCPfbO/TzJYHa53fzYZr+vksWR1fXasR5rxy65EpR5N0rSBXT3O+XzySIXXiND9z//bvc8v0CW/QJdKh92ooHIVNGvIw0r+aZfKVaudb58FAC4UHFxWknTs2DGVLft/a89+//24ypQJ9lZZAK4BBaLRePjhh3X69GlVrVpV/v7+ORaD//7775c81+l0yun0nCblW+zS44G8ON9knPjtf+r67Fj5lfBM3hp06K46Tdt77Fvw0uNq+uDjqly30aUv/P+TjOyzWcZrBoC8uOHGGxUcXFYb1v1XtWr9+YuPrMxMJXy3WU8PHurl6gAUZgWi0ZgwYYK3S8B1KvNMulKP/N8c5NSjyTp66Cc5AwJVolQZLXt7pI4c3KdOMa/Kss4pLfXPJrZ4QKB8ivoqwBV00QXggWVC5CobJklK/nm3fvt5j8JrRMjpX0KpR5O0YclcuULKKaxqrfz5oACua6fT0nTo0CH3z/87fFi7d+2Sy+VSufBwPdTrEc2cMV0VKlZShYoVNfOd6SpevLju7tDRfc6xo0d17Ngx/fL/r7Nv74/y9w9QuXLl5CpVKr8/EnBVXetTmvKL1xuNrKwsrV69Wi+99BIv7UO+O3LgR3005ln3z98unC5JqtXkLjXq8rB+TtwgSXpv+ACP8+57bqxuvOmWXN2jqK9T+xL+qw1L5ykr44wCSgWpYmQDtX/iBRX1LWbokwDApe3YsV2PPfqI++c3xsZKkjp1vlcjR43Wo/2ilZGRoVEjX9HJk6mKvPkWTZ0xSwEBJdzn/GfRQk2bMtn986OPPCRJevW1WHW+13P6KABIksMqAKtRS5Uqpe+//95YozFl3QEj1wGAgqLv7ZW8XQIAGFXc67/uvrRqQ7/It3vte6P95QcVUgXiqVP33nuvli5d6u0yAAAAABhSIHrJatWqaeTIkVq3bp3q16+vgIAAj+NPPfWUlyoDAADA9YY1GmYUiEbj3XffValSpZSQkKCEhASPYw6Hg0YDAAAAKGQKRKOxf/9+b5cAAAAASBIvqzSkQKzRsLMsi7clAwAAAIVcgWk05s6dq8jISPn5+cnPz08333yz5s2b5+2yAAAAcJ1xOBz5tl3LCsTUqXHjxumll17Sk08+qSZNmsiyLP33v//VP/7xDx07dkz//Oc/vV0iAAAAgDwoEI3GpEmTNHXqVD3yyP+9TKhz586qU6eORowYQaMBAACAfHONBw35pkBMnUpKSlLjxo1z7G/cuLGSkpK8UBEAAACAv6NANBrVqlXTokWLcuz/4IMPVL16dS9UBAAAgOtVkSKOfNuuZQVi6tQrr7yi7t2765tvvlGTJk3kcDi0du1aff311xdtQAAAAAAUbAWi0bjvvvu0ceNGjRs3TkuXLpVlWapdu7Y2bdqkevXqebs8AAAAXEdYo2FGgWg0JKl+/fpasGCBt8sAAAAAYIBXG40iRYpc9vnBDodDZ8+ezaeKAAAAcL271t9vkV+82mgsWbLkksfWrVunSZMm8ZZwAAAAoBDyaqPRuXPnHPt2796tYcOG6dNPP9VDDz2kkSNHeqEyAAAAAH9HgXi8rST9+uuvio6O1s0336yzZ88qMTFRc+bMUYUKFbxdGgAAAK4jDkf+bdcyrzcaqampeu6551StWjXt2LFDX3/9tT799FNFRER4uzQAAAAAV8irU6fGjh2rMWPGKCwsTO+///5Fp1IBAAAA+YnF4GZ4tdF4/vnn5efnp2rVqmnOnDmaM2fORcctXrw4nysDAAAA8Hd4tdF45JFH6BgBAABQoPDvUzO82mjExcV58/YAAAAArpIC82ZwAAAAoCAg0DDD60+dAgAAAHDtIdEAAAAAbFijYQaJBgAAAADjSDQAAAAAGwINM0g0AAAAABhHogEAAADYsEbDDBINAAAAAMaRaAAAAAA2BBpmkGgAAAAAMI5EAwAAALBhjYYZJBoAAAAAjCPRAAAAAGwINMwg0QAAAABgHI0GAAAAAOOYOgUAAADYsBjcDBINAAAAAMaRaAAAAAA2BBpmkGgAAAAAMI5GAwAAALBxOBz5tuVFbGysbrvtNgUGBiokJERdunTRnj17PMZYlqURI0YoPDxcfn5+at68uXbs2OExJiMjQ4MGDVJwcLACAgLUqVMnHT582GNMSkqKevXqJZfLJZfLpV69eunEiRN5qpdGAwAAACgE1qxZo4EDB2rDhg1avny5zp49qzZt2igtLc09ZuzYsRo3bpwmT56szZs3KywsTHfddZf++OMP95iYmBgtWbJECxcu1Nq1a3Xq1Cl17NhR2dnZ7jE9e/ZUYmKi4uPjFR8fr8TERPXq1StP9Tosy7L+/scuWKasO+DtEgDAqL63V/J2CQBgVPECvFK48dhv8u1e655tesXnHj16VCEhIVqzZo2aNm0qy7IUHh6umJgYPffcc5L+TC9CQ0M1ZswYPf7440pNTVXZsmU1b948de/eXZL066+/qnz58lq2bJnatm2rXbt2qXbt2tqwYYMaNmwoSdqwYYOioqK0e/du1axZM1f1kWgAAAAAXpKRkaGTJ096bBkZGbk6NzU1VZIUFBQkSdq/f7+Sk5PVpk0b9xin06lmzZpp3bp1kqSEhARlZWV5jAkPD1dERIR7zPr16+VyudxNhiQ1atRILpfLPSY3aDQAAAAAm/xcoxEbG+teB3F+i42NvWyNlmVp8ODBuuOOOxQRESFJSk5OliSFhoZ6jA0NDXUfS05OVrFixVS6dOm/HBMSEpLjniEhIe4xuVGAQysAAADg2jZs2DANHjzYY5/T6bzseU8++aS2bt2qtWvX5jh24SJzy7Iuu/D8wjEXG5+b69iRaAAAAAA2Dkf+bU6nUyVLlvTYLtdoDBo0SJ988olWrVqlG2+80b0/LCxMknKkDkeOHHGnHGFhYcrMzFRKSspfjvntt99y3Pfo0aM50pK/QqMBAAAAFAKWZenJJ5/U4sWLtXLlSlWuXNnjeOXKlRUWFqbly5e792VmZmrNmjVq3LixJKl+/fry9fX1GJOUlKTt27e7x0RFRSk1NVWbNm1yj9m4caNSU1PdY3KDqVMAAACATV7fb5FfBg4cqPfee08ff/yxAgMD3cmFy+WSn5+fHA6HYmJiNGrUKFWvXl3Vq1fXqFGj5O/vr549e7rH9uvXT0OGDFGZMmUUFBSkoUOHKjIyUq1bt5Yk1apVS+3atVN0dLSmT58uSerfv786duyY6ydOSTQaAAAAQKEwdepUSVLz5s099s+ePVt9+vSRJD377LNKT0/XgAEDlJKSooYNG+qrr75SYGCge/z48eNVtGhRdevWTenp6WrVqpXi4uLk4+PjHrNgwQI99dRT7qdTderUSZMnT85TvbxHAwAKAd6jAeBaU5Dfo9F03H/z7V7fDG6Sb/fKb6zRAAAAAGBcAe4lAQAAgPxXQJdoFDokGgAAAACMo9EAAAAAYBxTpwAAAACbgvp428KGRAMAAACAcSQaAAAAgA2BhhkkGgAAAACMI9EAAAAAbFijYQaJBgAAAADjSDQAAAAAGwINM0g0AAAAABhHogEAAADYFCHSMIJEAwAAAIBxJBoAAACADYGGGSQaAAAAAIwj0QAAAABseI+GGSQaAAAAAIwj0QAAAABsihBoGEGiAQAAAMA4Eg0AAADAhjUaZpBoAAAAADCORAMAAACwIdAwg0QDAAAAgHE0GgAAAACMY+oUAAAAYOMQc6dMINEAAAAAYByJBgAAAGDDC/vMINEAAAAAYByJBgAAAGDDC/vMINEAAAAAYByJBgAAAGBDoGEGiQYAAAAA40g0AAAAAJsiRBpGkGgAAAAAMI5EAwAAALAh0DCDRAMAAACAcSQaAAAAgA3v0TCDRAMAAACAcSQaAAAAgA2BhhkkGgAAAACMI9EAAAAAbHiPhhkkGgAAAACMo9EAAAAAYBxTpwAAAAAbJk6ZQaIBAAAAwDgSDQAAAMCGF/aZQaIBAAAAwDgSDQAAAMCmCIGGESQaAAAAAIwj0QAAAABsWKNhBokGAAAAAONINAAAAAAbAg0zSDQAAAAAGEeiAQAAANiwRsMMEg0AAAAAxpFoAAAAADa8R8MMEg0AAAAAxpFoAAAAADas0TCDRAMAAACAcSQaAAAAgA15hhkkGgAAAACMI9EAAAAAbIqwRsMIEg0AAAAAxtFoAAAAADDuihqNefPmqUmTJgoPD9fBgwclSRMmTNDHH39stDgAAAAgvzkc+bddy/LcaEydOlWDBw/W3XffrRMnTig7O1uSVKpUKU2YMMF0fQAAAAAKoTw3GpMmTdKMGTP04osvysfHx72/QYMG2rZtm9HiAAAAgPzmcDjybbuW5bnR2L9/v+rVq5djv9PpVFpampGiAAAAABRueW40KleurMTExBz7v/jiC9WuXdtETQAAAIDXsEbDjDy/R+OZZ57RwIEDdebMGVmWpU2bNun9999XbGys3n333atRIwAAAIBCJs+NxqOPPqqzZ8/q2Wef1enTp9WzZ0/dcMMNeuutt9SjR4+rUSMAAACQb3hhnxlX9Gbw6OhoRUdH69ixYzp37pxCQkJM1wUAAACgELuiRuO84OBgU3UAAAAABQKBhhl5bjQqV678l4/i+vnnn/9WQQAAAAAKvzw3GjExMR4/Z2VlacuWLYqPj9czzzxjqi4AAADAK67191vklzw3Gk8//fRF97/99tv67rvv/nZBAAAAAAo/h2VZlokL/fzzz6pbt65Onjxp4nJ/S8rpbG+XAABGhTe5+C95AKCwSt8y2dslXNKgJbvy7V6T7q2Vb/fKb3l+Yd+lfPjhhwoKCjJ1OQAAAACFWJ6nTtWrV89j3pplWUpOTtbRo0c1ZcoUo8UBAAAA+Y01GmbkudHo0qWLx89FihRR2bJl1bx5c910002m6gIAAABQiOWp0Th79qwqVaqktm3bKiws7GrVBAAAAHhNEQINI/K0RqNo0aJ64oknlJGRcbXqAQAAAHAR33zzje655x6Fh4fL4XBo6dKlHsf79Okjh8PhsTVq1MhjTEZGhgYNGqTg4GAFBASoU6dOOnz4sMeYlJQU9erVSy6XSy6XS7169dKJEyfyXG+eF4M3bNhQW7ZsyfONAAAAAFy5tLQ03XLLLZo8+dJP7GrXrp2SkpLc27JlyzyOx8TEaMmSJVq4cKHWrl2rU6dOqWPHjsrO/r+ntvbs2VOJiYmKj49XfHy8EhMT1atXrzzXm+c1GgMGDNCQIUN0+PBh1a9fXwEBAR7Hb7755jwXAQAAABQUBXXqVPv27dW+ffu/HON0Oi+5xCE1NVUzZ87UvHnz1Lp1a0nS/PnzVb58ea1YsUJt27bVrl27FB8frw0bNqhhw4aSpBkzZigqKkp79uxRzZo1c11vrhuNvn37asKECerevbsk6amnnnIfczgcsixLDofDoxsCAAAAcGkZGRk5liU4nU45nc4rut7q1asVEhKiUqVKqVmzZnr99dcVEhIiSUpISFBWVpbatGnjHh8eHq6IiAitW7dObdu21fr16+VyudxNhiQ1atRILpdL69aty1OjkeupU3PmzNGZM2e0f//+HNvPP//s/v8AAABAYXbhOoerucXGxrrXQpzfYmNjr6ju9u3ba8GCBVq5cqXefPNNbd68WS1btnQ3MsnJySpWrJhKly7tcV5oaKiSk5PdY843JnYhISHuMbmV60Tj/AvEK1asmKcbAAAAALi4YcOGafDgwR77rjTNOD/zSJIiIiLUoEEDVaxYUZ9//rm6du16yfPOz0w672LvEblwTG7kaY0GLy8BAADAtS4/12j8nWlSl1OuXDlVrFhRe/fulSSFhYUpMzNTKSkpHqnGkSNH1LhxY/eY3377Lce1jh49qtDQ0DzdP09PnapRo4aCgoL+cgMAAADgfcePH9cvv/yicuXKSZLq168vX19fLV++3D0mKSlJ27dvdzcaUVFRSk1N1aZNm9xjNm7cqNTUVPeY3MpTovHKK6/I5XLl6QYAAABAYVJQJ/GcOnVK+/btc/+8f/9+JSYmun/hP2LECN13330qV66cDhw4oBdeeEHBwcG69957JUkul0v9+vXTkCFDVKZMGQUFBWno0KGKjIx0P4WqVq1aateunaKjozV9+nRJUv/+/dWxY8c8LQSX8tho9OjR46KLQwAAAABcXd99951atGjh/vn82o7evXtr6tSp2rZtm+bOnasTJ06oXLlyatGihT744AMFBga6zxk/fryKFi2qbt26KT09Xa1atVJcXJx8fHzcYxYsWKCnnnrK/XSqTp06/eW7Oy7FYZ1f5X0ZPj4+SkpKKhSNRsppHrEL4NoS3uRpb5cAAEalb8n7P1zzy/PLfsy3e42+u0a+3Su/5XqNRi77EQAAAADI/dSpc+fOXc06AAAAgAIhT09LwiXxPQIAAAAwLk+LwQEAAIBrXUF96lRhQ6IBAAAAwDgSDQAAAMCmCJGGESQaAAAAAIwj0QAAAABsCDTMINEAAAAAYByJBgAAAGBThETDCBINAAAAAMbRaAAAAAAwjqlTAAAAgA2PtzWDRAMAAACAcSQaAAAAgA2BhhkkGgAAAACMI9EAAAAAbHi8rRkkGgAAAACMI9EAAAAAbBwi0jCBRAMAAACAcSQaAAAAgA1rNMwg0QAAAABgHIkGAAAAYEOiYQaJBgAAAADjSDQAAAAAGwevBjeCRAMAAACAcSQaAAAAgA1rNMwg0QAAAABgHIkGAAAAYMMSDTNINAAAAAAYR6MBAAAAwDimTgEAAAA2RZg7ZQSJBgAAAADjSDQAAAAAGx5vawaJBgAAAADjSDQAAAAAG5ZomEGiAQAAAMA4Eg0AAADApoiINEwg0QAAAABgHIkGAAAAYMMaDTNINAAAAAAYR6IBAAAA2PAeDTNINAAAAAAYR6IBAAAA2BRhkYYRJBoAAAAAjCPRAAAAAGwINMwg0QAAAABgHIkGAAAAYMMaDTNINAAAAAAYR6IBAAAA2BBomEGiAQAAAMA4Gg0AAAAAxjF1CgAAALDhN/Fm8D0CAAAAMI5EAwAAALBxsBrcCBINAAAAAMaRaAAAAAA25BlmkGgAAAAAMI5EAwAAALApwhoNI0g0AAAAABhHogEAAADYkGeYQaIBAAAAwDgSDQAAAMCGJRpmkGgAAAAAMI5EAwAAALDhzeBmkGgAAAAAMI5EAwAAALDhN/Fm8D0CAAAAMI5EAwAAALBhjYYZJBoAAAAAjKPRAAAAAGAcU6cAAAAAGyZOmUGiAQAAAMA4Eg0AAADAhsXgZpBoAAAAADCORAMAAACw4TfxZvA9AgAAADCORAMAAACwYY2GGSQaAAAAAIwj0QAAAABsyDPMINEAAAAAYByJBgAAAGDDEg0zSDQAAACAQuCbb77RPffco/DwcDkcDi1dutTjuGVZGjFihMLDw+Xn56fmzZtrx44dHmMyMjI0aNAgBQcHKyAgQJ06ddLhw4c9xqSkpKhXr15yuVxyuVzq1auXTpw4ked6aTQAAAAAmyJy5NuWF2lpabrllls0efLkix4fO3asxo0bp8mTJ2vz5s0KCwvTXXfdpT/++MM9JiYmRkuWLNHChQu1du1anTp1Sh07dlR2drZ7TM+ePZWYmKj4+HjFx8crMTFRvXr1yvP36LAsy8rzWQVcyunsyw8CgEIkvMnT3i4BAIxK33LxfywXBJ9u+y3f7nVPZOgVnedwOLRkyRJ16dJF0p9pRnh4uGJiYvTcc89J+jO9CA0N1ZgxY/T4448rNTVVZcuW1bx589S9e3dJ0q+//qry5ctr2bJlatu2rXbt2qXatWtrw4YNatiwoSRpw4YNioqK0u7du1WzZs1c10iiAQAAANg4HPm3ZWRk6OTJkx5bRkZGnmvev3+/kpOT1aZNG/c+p9OpZs2aad26dZKkhIQEZWVleYwJDw9XRESEe8z69evlcrncTYYkNWrUSC6Xyz0mt2g0AAAAAC+JjY11r4U4v8XGxub5OsnJyZKk0FDPhCQ0NNR9LDk5WcWKFVPp0qX/ckxISEiO64eEhLjH5BZPnQIAAABsHPn4Jo1hw4Zp8ODBHvucTucVX+/Ct5pblnXZN51fOOZi43NznQuRaAAAAABe4nQ6VbJkSY/tShqNsLAwScqROhw5csSdcoSFhSkzM1MpKSl/Oea333KuUTl69GiOtORyaDQAAAAAm/xco2FK5cqVFRYWpuXLl7v3ZWZmas2aNWrcuLEkqX79+vL19fUYk5SUpO3bt7vHREVFKTU1VZs2bXKP2bhxo1JTU91jcoupUwAAAEAhcOrUKe3bt8/98/79+5WYmKigoCBVqFBBMTExGjVqlKpXr67q1atr1KhR8vf3V8+ePSVJLpdL/fr105AhQ1SmTBkFBQVp6NChioyMVOvWrSVJtWrVUrt27RQdHa3p06dLkvr376+OHTvm6YlTEo0GAAAAUCh89913atGihfvn82s7evfurbi4OD377LNKT0/XgAEDlJKSooYNG+qrr75SYGCg+5zx48eraNGi6tatm9LT09WqVSvFxcXJx8fHPWbBggV66qmn3E+n6tSp0yXf3fFXeI8GABQCvEcDwLWmIL9HI37H0Xy7V7s6ZfPtXvmNNRoAAAAAjGPqFAAAAGBjcpH29YxEAwAAAIBxJBoAAACADYmGGSQaAAAAAIwj0QAAAABsHCLSMIFEAwAAAIBxJBoAAACATRECDSNINAAAAAAYR6IBAAAA2LBGwwwSDQAAAADGkWgAAAAANrxHwwwSDQAAAADGkWgAAAAANqzRMINEAwAAAIBxJBoAAACADe/RMINEAwAAAIBxNBoAAAAAjGPqFAAAAGDDYnAzvN5oWJalhIQEHThwQA6HQ5UrV1a9evXk4AHGAAAAQKHl1UZj1apV6tevnw4ePCjLsiTJ3WzMmjVLTZs29WZ5AAAAuA7x+24zvNZo7Nu3Tx07dlTDhg01fvx43XTTTbIsSzt37tTEiRN19913a+vWrapSpYq3SsR1aM7Md7R65QodPPCznM7iirylrgY+PUQVK1W+6PjRrw3X0o/+o5ihz6vHQ4+49y/9aJG+/OJz7dm9U6fT0rT8mw0KDCyZXx8DwHVsaN826tLyFtWoFKr0jCxt/OFnvfjWx9p78Ih7TEhQoF57urNaR9WSq4Sf1n6/T4PH/kc/HTrqHlP5xmCN/ue9iqpXRU7folq+bpcGj/mPjvz+h3vMfyY8rltq3KCyQYFKOXlaqzbu0b8mfqyko6n5+pkBFExeWww+YcIENWrUSCtXrlTnzp1Vs2ZN3XTTTeratatWrVrlbkCA/LTl++90X/cH9e7c9zVx6rvKzs7W0088pvT00znGrlm1Qju2bVXZsiE5jp05c0ZRje9Qn77986NsAHC789ZqmvbBN2r2yBvq+MRk+fj46LOpT8q/eDH3mEXj+6vyjcF6IGa6Gj04WoeSfteyaYPcY/yLF9NnUwbKsiy17z9JLR8dr2K+Pvrorcc9pjZ/s/lHPfzcLN1y76vq+cy7qlI+WO/9u1++f2bANEc+btcyrzUaq1evVkxMzEWPORwOxcTEaNWqVflbFK57E95+Rx073asqVaures2b9K8Rrys5OUm7d+70GHfkyG96Y/TremXUWPkUzRkM9njoET3SN1p1br4lv0oHAElS5yenaP6nG7Xr52Rt+/F/enzEfFUoF6R6tctLkqpVCFHDmyvrqdcXKmHnIe09eERPx36gAD+nurWvL0mKqltFFcPLKHr4fO3Y96t27PtV/YfPV4OISmp+ew33vSYtWKVN2w7oUFKKNvywX2/MXq7bIyupaFEeagnAi43GoUOHFBkZecnjEREROnjwYD5WBOR06tSfUwRKulzufefOndMr/3peD/fuqypVq3urNADIlZIlikuSUlL/TGadxf785ciZzLPuMefOWcrMOqvGdau6x1iWpQzbmDOZZ5Wdfc495kKlS/qrR/sG2vDDfp09e+6qfBYgvxRxOPJtu5Z5rdE4deqU/P39L3nc399fp0/nnK5yoYyMDJ08edJjy8jIMFkqrlOWZemtN8fqlnq3qmq1/2so5s1+Vz4+Pur24MNerA4AcmfMkPv03+/3aedPSZKkPQeSdfDX4xo5qJNKBfrJt6iPhj56l8qVdSks+M9fqmzadkBp6Zl6/enO8ivuK//ixRQb00U+PkUUFuy53uy1pzrr2Lo39euasSpfLkgP/POdfP+MAAomr2abO3fu1NatWy+67dixI1fXiI2Nlcvl8tjGvzH6KleO68Ebo1/Tvr17NDL2Dfe+3Tt36IP35+mlV0bxCGYABd7457spsnq4eg+Lc+87e/acHhz6rqpVDFHSN//W7+vH6c761RW/doeyz/2ZRBxLOaWHnp2pu5tG6Nh/39Rv3/5bJUv46fudh9xj3PeYu0KNeoxRh39MVnb2Ob07sld+fkTgqmCNhhlefbxtq1at3I+1vZjc/ENu2LBhGjx4sMe+09lefz0ICrk3Rr+mb9es0rSZcxUSGuben7glQSm//64ud7dy78vOztbEcWO1cMFcLV22whvlAkAO4557QB2bRap1vwn635ETHse27PpFjXqMVskSxVXMt6iOpZzSN3OHKmHnIfeYrzfsVp1Or6hMqQCdPXtOqafStX/5KB3833GPax0/kabjJ9K079AR7dmfrH1fvqaGN1fWxq378+NjAijAvPYv8v37L/8XUEpKymXHOJ1OOZ1Oj33Zp7OvuC5c3yzL0ptjXtealSv09ow4hd9wo8fx9h066baGUR77YgZEq12HTurY+d78LBUALmn8cw+oU8tb1Cb6LR389fglx508dUaSVLVCWd1au4JemfJZjjHHT6RJkprdVkMhQSX02Zptl7ze+d8PFvPlF34o5K71qCGfeO1vgooVK150f2pqqhYsWKCZM2cqMTFR2dk0Dcg//44dqa+++Fxjx09WQECAjh/785nyASUCVbx4cblKlZKrVCmPc3yKFlWZ4GCPd20cP3ZUx48f0+FDf/528Ke9P8o/IEChYeXkcnmeDwAmTRjWTd3bN9AD/3xHp9LOKLRMoCQp9dQZncnIkiR1bV1PR1NO6Zfk3xVRPVxvPHO/Pl29VV9v2O2+Tq9OjbRnf7KOppxSw5sr641n7tekBavc7+NoUKeiGkRU1LotP+nEH6dV6YZgvfxEB/106ChpBgBJXp46Zbdy5UrNmjVLixcvVsWKFXXffffp3Xff9XZZuM4s/s9CSdKA6N4e+//1yuvq2Cn3icXiDz/QzOlT3D//o98jV3QdAMirx7s1lSQtfzfGY3/0y/M0/9ONkqSwsiU1ZkhXhZQJVPKxk1rw2UbFvhPvMb5GpRC9OqiTglz+Ovjr7xo780tNnL/SfTw9I0udW96if/2jgwL8iin5WKq+WrdLjzw/W5lZZwUUZg4iDSMc1l8tkrjKDh8+rLi4OM2aNUtpaWnq1q2bpk2bph9++EG1a9e+4uumMHUKwDUmvMnT3i4BAIxK3zLZ2yVc0saf8u/t9g2rui4/qJDy2lOn7r77btWuXVs7d+7UpEmT9Ouvv2rSpEneKgcAAACQ9Od6o/zarmVemzr11Vdf6amnntITTzyh6tV56RkAAABwLfFaovHtt9/qjz/+UIMGDdSwYUNNnjxZR48e9VY5AAAAgCTeo2GK1xqNqKgozZgxQ0lJSXr88ce1cOFC3XDDDTp37pyWL1+uP/74w1ulAQAAAPibvPpmcEny9/dX3759tXbtWm3btk1DhgzR6NGjFRISok6dOnm7PAAAAFxviDSM8HqjYVezZk2NHTtWhw8f1vvvv+/tcgAAAABcoQLVaJzn4+OjLl266JNPPvF2KQAAAACuQIF5YR8AAABQEPDCPjMKZKIBAAAAoHAj0QAAAABsrvUX6eUXEg0AAAAAxpFoAAAAADYEGmaQaAAAAAAwjkQDAAAAsCPSMIJEAwAAAIBxJBoAAACADe/RMINEAwAAAIBxJBoAAACADe/RMINEAwAAAIBxJBoAAACADYGGGSQaAAAAAIwj0QAAAADsiDSMINEAAAAAYByJBgAAAGDDezTMINEAAAAAYByNBgAAAADjmDoFAAAA2PDCPjNINAAAAAAYR6IBAAAA2BBomEGiAQAAAMA4Eg0AAADAjkjDCBINAAAAAMaRaAAAAAA2vLDPDBINAAAAAMaRaAAAAAA2vEfDDBINAAAAAMaRaAAAAAA2BBpmkGgAAAAAMI5EAwAAALAj0jCCRAMAAACAcSQaAAAAgA3v0TCDRAMAAACAcSQaAAAAgA3v0TCDRAMAAACAcTQaAAAAAIxj6hQAAABgw8wpM0g0AAAAABhHogEAAADYEWkYQaIBAAAAwDgSDQAAAMCGF/aZQaIBAAAAFAIjRoyQw+Hw2MLCwtzHLcvSiBEjFB4eLj8/PzVv3lw7duzwuEZGRoYGDRqk4OBgBQQEqFOnTjp8+PBVqZdGAwAAALBxOPJvy6s6deooKSnJvW3bts19bOzYsRo3bpwmT56szZs3KywsTHfddZf++OMP95iYmBgtWbJECxcu1Nq1a3Xq1Cl17NhR2dnZJr46D0ydAgAAAAqJokWLeqQY51mWpQkTJujFF19U165dJUlz5sxRaGio3nvvPT3++ONKTU3VzJkzNW/ePLVu3VqSNH/+fJUvX14rVqxQ27ZtjdZKogEAAADYOPJxy8jI0MmTJz22jIyMS9a2d+9ehYeHq3LlyurRo4d+/vlnSdL+/fuVnJysNm3auMc6nU41a9ZM69atkyQlJCQoKyvLY0x4eLgiIiLcY0yi0QAAAAC8JDY2Vi6Xy2OLjY296NiGDRtq7ty5+vLLLzVjxgwlJyercePGOn78uJKTkyVJoaGhHueEhoa6jyUnJ6tYsWIqXbr0JceYxNQpAAAAwC4fHzo1bNgwDR482GOf0+m86Nj27du7/xwZGamoqChVrVpVc+bMUaNGjSRJjgsWfliWlWPfhXIz5kqQaAAAAABe4nQ6VbJkSY/tUo3GhQICAhQZGam9e/e6121cmEwcOXLEnXKEhYUpMzNTKSkplxxjEo0GAAAAYOPIx//7OzIyMrRr1y6VK1dOlStXVlhYmJYvX+4+npmZqTVr1qhx48aSpPr168vX19djTFJSkrZv3+4eYxJTpwAAAIBCYOjQobrnnntUoUIFHTlyRK+99ppOnjyp3r17y+FwKCYmRqNGjVL16tVVvXp1jRo1Sv7+/urZs6ckyeVyqV+/fhoyZIjKlCmjoKAgDR06VJGRke6nUJlEowEAAADYXIXlCkYcPnxYDz74oI4dO6ayZcuqUaNG2rBhgypWrChJevbZZ5Wenq4BAwYoJSVFDRs21FdffaXAwED3NcaPH6+iRYuqW7duSk9PV6tWrRQXFycfHx/j9Tosy7KMX9XLUk6bf+EIAHhTeJOnvV0CABiVvmWyt0u4pP3HzuTbvSoHF8+3e+U3Eg0AAADApoAGGoUOi8EBAAAAGEeiAQAAANgRaRhBogEAAADAOBoNAAAAAMYxdQoAAACw+bsv0sOfSDQAAAAAGEeiAQAAANgU1Bf2FTYkGgAAAACMI9EAAAAAbAg0zCDRAAAAAGAciQYAAABgwxoNM0g0AAAAABhHogEAAAB4INIwgUQDAAAAgHEkGgAAAIANazTMINEAAAAAYByJBgAAAGBDoGEGiQYAAAAA40g0AAAAABvWaJhBogEAAADAOBINAAAAwMbBKg0jSDQAAAAAGEejAQAAAMA4pk4BAAAAdsycMoJEAwAAAIBxJBoAAACADYGGGSQaAAAAAIwj0QAAAABseGGfGSQaAAAAAIwj0QAAAABseGGfGSQaAAAAAIwj0QAAAADsCDSMINEAAAAAYByJBgAAAGBDoGEGiQYAAAAA40g0AAAAABveo2EGiQYAAAAA40g0AAAAABveo2EGiQYAAAAA40g0AAAAABvWaJhBogEAAADAOBoNAAAAAMbRaAAAAAAwjkYDAAAAgHEsBgcAAABsWAxuBokGAAAAAONINAAAAAAbXthnBokGAAAAAONINAAAAAAb1miYQaIBAAAAwDgSDQAAAMCGQMMMEg0AAAAAxpFoAAAAAHZEGkaQaAAAAAAwjkQDAAAAsOE9GmaQaAAAAAAwjkQDAAAAsOE9GmaQaAAAAAAwjkQDAAAAsCHQMINEAwAAAIBxJBoAAACAHZGGESQaAAAAAIyj0QAAAABgHFOnAAAAABte2GcGiQYAAAAA40g0AAAAABte2GcGiQYAAAAA4xyWZVneLgIojDIyMhQbG6thw4bJ6XR6uxwA+Nv4ew2ASTQawBU6efKkXC6XUlNTVbJkSW+XAwB/G3+vATCJqVMAAAAAjKPRAAAAAGAcjQYAAAAA42g0gCvkdDo1fPhwFkwCuGbw9xoAk1gMDgAAAMA4Eg0AAAAAxtFoAAAAADCORgMAAACAcTQaQAGzevVqORwOnThxwtulAAAAXDEaDVzT+vTpI4fDodGjR3vsX7p0qRwOh5eqAgDvWrdunXx8fNSuXTuP/QcOHJDD4XBvgYGBqlOnjgYOHKi9e/d6qVoAhRWNBq55xYsX15gxY5SSkmLsmpmZmcauBQD5bdasWRo0aJDWrl2rQ4cO5Ti+YsUKJSUl6YcfftCoUaO0a9cu3XLLLfr666+9UC2AwopGA9e81q1bKywsTLGxsZcc89FHH6lOnTpyOp2qVKmS3nzzTY/jlSpV0muvvaY+ffrI5XIpOjpacXFxKlWqlD777DPVrFlT/v7+uv/++5WWlqY5c+aoUqVKKl26tAYNGqTs7Gz3tebPn68GDRooMDBQYWFh6tmzp44cOXLVPj8A2KWlpWnRokV64okn1LFjR8XFxeUYU6ZMGYWFhalKlSrq3LmzVqxYoYYNG6pfv34ef58BwF+h0cA1z8fHR6NGjdKkSZN0+PDhHMcTEhLUrVs39ejRQ9u2bdOIESP00ksv5fgf33//+9+KiIhQQkKCXnrpJUnS6dOnNXHiRC1cuFDx8fFavXq1unbtqmXLlmnZsmWaN2+e3nnnHX344Yfu62RmZmrkyJH64YcftHTpUu3fv199+vS5ml8BALh98MEHqlmzpmrWrKmHH35Ys2fP1uVeqVWkSBE9/fTTOnjwoBISEvKpUgCFXVFvFwDkh3vvvVd169bV8OHDNXPmTI9j48aNU6tWrdzNQ40aNbRz5079+9//9mgAWrZsqaFDh7p/Xrt2rbKysjR16lRVrVpVknT//fdr3rx5+u2331SiRAnVrl1bLVq00KpVq9S9e3dJUt++fd3XqFKliiZOnKjbb79dp06dUokSJa7WVwAAkqSZM2fq4YcfliS1a9dOp06d0tdff63WrVv/5Xk33XSTpD/Xcdx+++1XvU4AhR+JBq4bY8aM0Zw5c7Rz506P/bt27VKTJk089jVp0kR79+71mCLQoEGDHNf09/d3NxmSFBoaqkqVKnk0DKGhoR5To7Zs2aLOnTurYsWKCgwMVPPmzSXpovOkAcCkPXv2aNOmTerRo4ckqWjRourevbtmzZp12XPPpx48SANAbpFo4LrRtGlTtW3bVi+88IJHUmFZVo7/4bzYNIKAgIAc+3x9fT1+djgcF9137tw5SX/OjW7Tpo3atGmj+fPnq2zZsjp06JDatm3LAnMAV93MmTN19uxZ3XDDDe59lmXJ19f3sg/M2LVrlySpcuXKV7VGANcOGg1cV0aPHq26deuqRo0a7n21a9fW2rVrPcatW7dONWrUkI+Pj9H77969W8eOHdPo0aNVvnx5SdJ3331n9B4AcDFnz57V3Llz9eabb6pNmzYex+677z4tWLBAHTt2vOi5586d08SJE1W5cmXVq1cvP8oFcA2g0cB1JTIyUg899JAmTZrk3jdkyBDddtttGjlypLp3767169dr8uTJmjJlivH7V6hQQcWKFdOkSZP0j3/8Q9u3b9fIkSON3wcALvTZZ58pJSVF/fr1k8vl8jh2//33a+bMme5G4/jx40pOTtbp06e1fft2TZgwQZs2bdLnn39u/BcwAK5drNHAdWfkyJEeU6NuvfVWLVq0SAsXLlRERIRefvllvfrqq1flSVBly5ZVXFyc/vOf/6h27doaPXq03njjDeP3AYALzZw5U61bt87RZEh/JhqJiYn6/fffJf35WPBy5copMjJSzz//vGrVqqWtW7eqRYsW+V02gELMYV3umXYAAAAAkEckGgAAAACMo9EAAAAAYByNBgAAAADjaDQAAAAAGEejAQAAAMA4Gg0AAAAAxtFoAAAAADCORgMAAACAcTQaAFDAjBgxQnXr1nX/3KdPH3Xp0iXf6zhw4IAcDocSExPz/d4AgMKPRgMAcqlPnz5yOBxyOBzy9fVVlSpVNHToUKWlpV3V+7711luKi4vL1ViaAwBAQVHU2wUAQGHSrl07zZ49W1lZWfr222/12GOPKS0tTVOnTvUYl5WVJV9fXyP3dLlcRq4DAEB+ItEAgDxwOp0KCwtT+fLl1bNnTz300ENaunSpe7rTrFmzVKVKFTmdTlmWpdTUVPXv318hISEqWbKkWrZsqR9++MHjmqNHj1ZoaKgCAwPVr18/nTlzxuP4hVOnzp07pzFjxqhatWpyOp2qUKGCXn/9dUlS5cqVJUn16tWTw+FQ8+bN3efNnj1btWrVUvHixXXTTTdpypQpHvfZtGmT6tWrp+LFi6tBgwbasmWLwW8OAHC9IdEAgL/Bz89PWVlZkqR9+/Zp0aJF+uijj+Tj4yNJ6tChg4KCgrRs2TK5XC5Nnz5drVq10o8//qigoCAtWrRIw4cP19tvv60777xT8+bN08SJE1WlSpVL3nPYsGGaMWOGxo8frzvuuENJSUnavXu3pD+bhdtvv10rVqxQnTp1VKxYMUnSjBkzNHz4cE2ePFn16tXTli1bFB0drYCAAPXu3VtpaWnq2LGjWrZsqfnz52v//v16+umnr/K3BwC4ltFoAMAV2rRpk9577z21atVKkpSZmal58+apbNmykqSVK1dq27ZtOnLkiJxOpyTpjTfe0NKlS/Xhhx+qf//+mjBhgvr27avHHntMkvTaa69pxYoVOVKN8/744w+99dZbmjx5snr37i1Jqlq1qu644w5Jct+7TJkyCgsLc583cuRIvfnmm+rataukP5OPnTt3avr06erdu7cWLFig7OxszZo1S/7+/qpTp44OHz6sJ554wvTXBgC4TjB1CgDy4LPPPlOJEiVUvHhxRUVFqWnTppo0aZIkqWLFiu5/6EtSQkKCTp06pTJlyqhEiRLubf/+/frpp58kSbt27VJUVJTHPS782W7Xrl3KyMhwNze5cfToUf3yyy/q16+fRx2vvfaaRx233HKL/P39c1UHAACXQ6IBAHnQokULTZ06Vb6+vgoPD/dY8B0QEOAx9ty5cypXrpxWr16d4zqlSpW6ovv7+fnl+Zxz585J+nP6VMOGDT2OnZ/iZVnWFdUDAMCl0GgAQB4EBASoWrVquRp76623Kjk5WUWLFlWlSpUuOqZWrVrasGGDHnnkEfe+DRs2XPKa1atXl5+fn77++mv3dCu782sysrOz3ftCQ0N1ww036Oeff9ZDDz100evWrl1b8+bNU3p6uruZ+as6AAC4HKZOAcBV0rp1a0VFRalLly768ssvdeDAAa1bt07/+te/9N1330mSnn76ac2aNUuzZs3Sjz/+qOHDh2vHjh2XvGbx4sX13HPP6dlnn9XcuXP1008/acOGDZo5c6YkKSQkRH5+foqPj9dvv/2m1NRUSX++BDA2NlZvvfWWfvzxR23btk2zZ8/WuHHjJEk9e/ZUkSJF1K9fP+3cuVPLli3TG2+8cZW/IQDAtYxGAwCuEofDoWXLlqlp06bq27evatSooR49eujAgQMKDQ2VJHXv3l0vv/yynnvuOdWvX18HDx687ALsl156SUOGDNHLL7+sWrVqqXv37jpy5IgkqWjRopo4caKmT5+u8PBwde7cWZL02GOP6d1331VcXJwiIyPVrFkzxcXFuR+HW6JECX366afauXOn6tWrpxdffFFjxoy5it8OAOBa57CYmAsAAADAMBINAAAAAMbRaAAAAAAwjkYDAAAAgHE0GgAAAACMo9EAAAAAYByNBgAAAADjaDQAAAAAGEejAQAAAMA4Gg0AAAAAxtFoAAAAADCORgMAAACAcf8PRJ5sEeumhQMAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Test Accuracy: 0.9252948885976409\n",
      "C:\\Users\\cocom\\AppData\\Local\\Temp\\ipykernel_21004\\1804581572.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('gfnet_adni_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GFNet(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (filter): GlobalFilter()\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (pre_logits): Identity()\n",
       "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (final_dropout): Identity()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trained_model = train_model(model, train_loader, val_loader, num_epochs=25, learning_rate=0.001)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = evaluate_model(trained_model, test_loader)\n",
    "\n",
    "# Save the model\n",
    "torch.save(trained_model.state_dict(), 'gfnet_adni_model2.pth')\n",
    "\n",
    "# Load the model for inference\n",
    "model.load_state_dict(torch.load('gfnet_adni_model.pth'))\n",
    "model.to(device)  # Ensure the model is on the GPU for inference\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Evaluate the model\n",
    "# model.load_state_dict(torch.load('gfnet_adni_model2.pth'))\n",
    "# test_accuracy = evaluate_model(model, test_loader)\n",
    "# model.to(device)  # Ensure the model is on the GPU for inference\n",
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3710",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
