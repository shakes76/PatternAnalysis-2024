"""
Contains the source code for components of the GFNet model.

Created by: Shogo Terashima
Created by:     Shogo Terashima
ID:             S47779628
Last update:    25/10/2024
Reference:
- Rao, Y., Zhao, W., Zhu, Z., Zhou, J., & Lu, J. (2023). 
    GFNet: Global Filter Networks for Visual Recognition. 
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(9), 10960–10973. 
    https://doi.org/10.1109/tpami.2023.3263824

- raoyongming. (2021). GitHub - raoyongming/GFNet: 
    [NeurIPS 2021] [T-PAMI] Global Filter Networks for Image Classification. GitHub.
     https://github.com/raoyongming/GFNet?tab=MIT-1-ov-file#readme

"""
import torch
import torch.nn as nn
import torch.fft
import torch.nn.functional as F
from timm.layers import DropPath

class GlobalFilterLayer(nn.Module):
    '''
    Applies a learnable global filter in the frequency domain, based on the Global Filter Layer pseudocode.
    Implemented based on the pseudocode of Global Filter Layer from the paper.
    It consists of Layer Normalisation, 2D FFT, Element-wise multiplication, and 2D IFFT.
    '''
    def __init__(self, height, width, dimension):
        super().__init__()
        # Define learnable global filters in the frequency domain
        w_halt = width // 2 + 1  # conjugate symmetry in FFT
        self.frequency_domain_filters = nn.Parameter(torch.randn(height, width, dimension, 2, dtype=torch.float32) * 0.02)

        self.layer_norm = nn.LayerNorm(dimension)

    def forward(self, x):
        '''
        layer norm -> 2D FFT -> element wise mult -> 2D IFFT 
        '''
        batch_size, N, channels = x.shape
        height = width = int(N ** 0.5)  # assuming N is a perfect square
        
        x = x.view(batch_size, height, width, channels)
        x_normalized = self.layer_norm(x)
        X = torch.fft.rfft2(x_normalized, dim=(1, 2), norm='ortho') # 2D FFT 
        X_tilde = X * torch.view_as_complex(self.frequency_domain_filters) # Element-wise multiplication
        x_filtered = torch.fft.irfft2(X_tilde, s=(height, width), dim=(1, 2), norm='ortho')  # inverse 2D FFT 
        x_filtered = x_filtered.reshape(batch_size, N, channels)
        return x_filtered 
class FeedForwardLayer(nn.Module):
    '''
    A feedforward layer consisting of layer normalization, two MLP layers with GELU activation, and dropout.
    '''
    def __init__(self, input_dim, drop=0.1):
        super().__init__()
        hidden_dim = input_dim * 4 # WE SET THE MLP EXPANSION RATIO TO 4 FOR ALL THE FEEDFORWARD NETWORKS (from paper)
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.activation = nn.GELU()
        self.fc2 = nn.Linear(hidden_dim, input_dim)
        self.dropout = nn.Dropout(drop)
        self.layer_norm = nn.LayerNorm(input_dim) 

    def forward(self, x):
        x = self.layer_norm(x)
        x = self.fc1(x)
        x = self.activation(x)
        x = self.dropout(x)
        x = self.fc2(x)
        x = self.dropout(x)
        return x

class Block(nn.Module):
    '''
     A block that combines a Global Filter Layer and a Feed Forward Network, with skip connections and drop path.
    '''
    def __init__(self, height, width, dimension, drop_rate=0.1, drop_path_rate=0.0, init_values=1e-5):
        super().__init__()
        self.global_filter = GlobalFilterLayer(height, width, dimension)
        self.feed_foward = FeedForwardLayer(input_dim=dimension, drop=drop_rate)
        self.drop_path = DropPath(drop_path_rate)
        self.gamma = nn.Parameter(init_values * torch.ones((dimension)), requires_grad=True)
    def forward(self, x):
        residual = x # use for skip connection
        x = self.global_filter(x) # Layer Norm -> 2D FFT -> Element-wise mult -> 2D IFFT
        x = self.feed_foward(x)  # Layer Norm -> MLP
        x = self.drop_path(self.gamma * x)        
        x = x + residual  # Skip connection
        
        return x
    

class PatchEmbedding(nn.Module):
    '''
    Embeds input images into patches for initial stages of the GFNet
    '''
    def __init__(self, dim_in=1, embed_dim=256, patch_size=4):
        super().__init__()
        self.patch_size = patch_size
        self.patch_to_embedding = nn.Conv2d(dim_in, embed_dim, kernel_size=self.patch_size, stride=self.patch_size)
        
    def forward(self, x):
        x = self.patch_to_embedding(x)  # (B, C, H, W) -> (B, embed_dim, H/patch_size, W/patch_size)
        x = x.flatten(2)  # -> (B, embed_dim, num_patches)
        x = x.transpose(1, 2) # -> (B, num_patches, embed_dim)
        return x
    
class DownSamplingLayer(nn.Module):
    '''
    Downsamples input feature maps for hierarchical GFNet stages.
    '''
    def __init__(self, dim_in, dim_out):
        super().__init__()
        self.dim_in = dim_in
        self.dim_out = dim_out
        self.downsample = nn.Conv2d(dim_in, dim_out, kernel_size=2, stride=2)

    def forward(self, x):
        batch_size, N, channels = x.size()
        height = width = int(N ** 0.5)  # assume 224 x 224
        x = x.view(batch_size, height, width, channels).permute(0, 3, 1, 2) # to (B, C, H, W)
        x = self.downsample(x).permute(0, 2, 3, 1) # to (B, dim_out, H/2, W/2)
        x = x.reshape(batch_size, -1, self.dim_out)  # (B, num_patches, dim_out)
        return x
    
class GFNet(nn.Module):
    '''
    Implements a hierarchical version of the GFNet model with Patch Embedding and DownSampling layers.
    Each stage starts with a Patch Embedding or DownSampling Layer, followed by a sequence of Blocks.
    After the last stage, the model performs global average pooling and outputs the final classification result through a linear layer.
    '''
    def __init__(self, image_size=224, num_classes=1, blocks_per_stage=[3, 3, 10, 3], 
                 stage_dims=[64, 128, 256, 512], drop_rate=0.1, drop_path_rate=0.1, init_values=0.001, dropcls=0.0):
        super().__init__()
        self.num_classes = num_classes
        self.patch_embedding = nn.ModuleList()
        self.position_embedding = nn.ParameterList() 
        
        patch_embedding = PatchEmbedding(dim_in=1, embed_dim=stage_dims[0], patch_size=4)
        num_patches = (image_size // 4) * (image_size // 4)
        self.patch_embedding.append(patch_embedding)
        self.position_embedding.append(nn.Parameter(torch.zeros(1, num_patches, stage_dims[0])))

        # Define DownSamplingLayers and patch embedding
        sizes = [56*image_size//224, 28*image_size//224, 14*image_size//224, 7*image_size//224] # Generally speaking, we can start from a large feature map (e.g., 56 × 56) and gradually perform downsampling after a few blocks. 

        for i in range(len(sizes)-1):
            patch_embedding = DownSamplingLayer(stage_dims[i], stage_dims[i+1])
            self.patch_embedding.append(patch_embedding)

        # storchastic drop path rate
        drop_path_probabilities = [x.item() for x in torch.linspace(0, drop_path_rate, sum(blocks_per_stage))]
        current_block_index = 0

        
        # Define the blocks for each stage
        self.blocks = nn.ModuleList()
        current_block_index = 0
        for stage_index in range(len(sizes)):
            height = sizes[stage_index]
            width = height // 2 + 1 
            num_blocks = blocks_per_stage[stage_index]
            blocks_for_current_stage = []
            for block_index in range(num_blocks):
                block = Block(
                    height=height,
                    width=width,
                    dimension=stage_dims[stage_index],
                    drop_rate=drop_rate,
                    drop_path_rate=drop_path_probabilities[current_block_index + block_index],
                    init_values=init_values
                )
                blocks_for_current_stage.append(block)
            self.blocks.append(nn.Sequential(*blocks_for_current_stage)) 
            current_block_index += num_blocks
        
        self.norm = nn.LayerNorm(stage_dims[-1])
        self.head = nn.Linear(stage_dims[-1], num_classes)  # fully connected  used for classification

    def forward(self, x):
        # Stage 1:
        x = self.patch_embedding[0](x)
        x = x + self.position_embedding[0]
        x = self.blocks[0](x)
        
        # Stage 2:
        x = self.patch_embedding[1](x)
        x = self.blocks[1](x)

        # Stage 3:
        x = self.patch_embedding[2](x)
        x = self.blocks[2](x)

        # Stage 4:
        x = self.patch_embedding[3](x)
        x = self.blocks[3](x)
        
        x = self.norm(x)
        x = x.mean(1) # Global average pooling
        x = self.head(x)
        return x
