"""
train.py created by Matthew Lockett 46988133

This file contains all the code necessary to train the StyleGAN model based on it's implementation 
in the modules.py file. All images from the ADNI dataset are loaded into this program, via a data loader function
described in the dataset.py file.

The following resources were a major help in developing the code for this training loop:
PyTorch DCGAN Tutorial: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html 
StyleGAN PyTorch Tutorial: https://blog.paperspace.com/implementation-stylegan-from-scratch/ 
ChatGPT 4o and o1-preview: The prompts utilised are referenced above the training loop.
"""
import os
import random
import time
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim
import torch.utils.data
import torchvision.utils as vutils
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import hyperparameters as hp
from dataset import load_ADNI_dataset
from modules import *
from utils import *

# Used to remove the Qt backend of matplotlib causing issues
matplotlib.use("Agg")

# Force the creation of a folder to save figures if not present 
os.makedirs(hp.SAVED_OUTPUT_DIR, exist_ok=True)

# PyTorch Device configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
if not torch.cuda.is_available():
    print("Warning CUDA not Found. Using CPU.")

# Set random seed for reproducibility
print("Random Seed Used: ", hp.RANDOM_SEED)
random.seed(hp.RANDOM_SEED)
torch.manual_seed(hp.RANDOM_SEED)
os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'
torch.use_deterministic_algorithms(True) # Needed for reproducible results

# Setup both the generator and discriminator models
gen = Generator().to(device)
disc = Discriminator().to(device)

# Create the optimisers used by the genertator and discriminator during training 
gen_opt = optim.Adam([{"params": [param for name, param in gen.named_parameters() if "mapping" not in name]},
                        {"params": gen.mapping.parameters(), "lr": 0.0001}], lr=hp.GEN_LEARNING_RATE, betas=(0.5, 0.999))
disc_opt = optim.Adam(disc.parameters(), lr=hp.DISC_LEARNING_RATE, betas=(0.5, 0.999))

# Initialise Loss Functions
adversarial_criterion = nn.BCELoss()
class_criterion = nn.CrossEntropyLoss()

# Initialise the real and fake labels for training
real_label = 1.0
fake_label = 0.0

# Create a collection of latent vectors used later to visualise the progression of the generator 
fixed_noise = torch.randn(64, hp.LATENT_SIZE, device=device)

# Lists to keep track of progress and statistics 
img_list = []
G_losses = []
D_losses = []
iters = 0

###################################### Training Loop ###################################

# Start time of the training loop
training_start_time = time.time()

# REF: This training loop was inspired by the following PyTorch tutorial: 
# REF: https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html?highlight=dcgan.
# REF: The OASIS GAN demo 2 for Matthew Lockett 46988133, was also used heavily as inspiration.
# REF: The mixing regularisation portion was inspired by code generated by ChatGPT-4o.
# REF: Based on the following prompt: How can I implement mixing regularization into my StyleGAN model?
# REF: The following code was also inspired by chatGPT-o1-preview via the following prompts:
# REF: Prompt: How should I write my discriminator to output both a rating for fake/real and a rating 
# REF: for class AD or CN while embedding the class information into the discriminator as well?
# REF: Prompt: How do I create a training loop that can be used for a Progressive GAN architecture
print("Starting Training Loop...")
for current_depth in range(hp.START_DEPTH, hp.MAX_DEPTH + 1):

    # Set the current resolution (starts at 8x8)
    current_resolution = 8 * (2 ** current_depth)
    print(f"\nTraining at resolution: {current_resolution}x{current_resolution}")

    # Load the ADNI dataset in at the current resolution
    train_loader = load_ADNI_dataset(image_size=current_resolution, training_set=True)

    real_batch = next(iter(train_loader))
    plt.figure(figsize=(8,8))
    plt.axis("off")
    plt.title("Sample Training Images for the ADNI Dataset")
    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))
    plt.savefig(os.path.join(hp.SAVED_OUTPUT_DIR, "adni_sample_images.png"), pad_inches=0)
    plt.close()

    # Reset the alpha and step counters
    alpha = hp.ALPHA_START
    total_steps = 0
    total_steps_per_resolution = (len(train_loader) * hp.EPOCHS_PER_RESOLUTION[current_depth])
    fade_in_steps = int(total_steps_per_resolution * hp.FADE_IN_PERCENTAGE)

    # Generate a new learning rate scheduler for each resolution
    gen_scheduler = exponential_decay_with_warmup(gen_opt, warmup_steps=total_steps_per_resolution*0.05, total_steps=total_steps_per_resolution)
    disc_scheduler = exponential_decay_with_warmup(disc_opt, warmup_steps=total_steps_per_resolution*0.05, total_steps=total_steps_per_resolution)

    # Loop over the total number of epochs per resolution
    for epoch in range(hp.EPOCHS_PER_RESOLUTION[current_depth]):

        # Start time of the epoch
        epoch_start_time = time.time()

        # Reset the data loader iterator
        data_iter = iter(train_loader)

        # Loop over each batch within the epoch 
        for i, (real_images, class_labels) in enumerate(data_iter):

            # Update alpha for fade-in phase
            if total_steps < fade_in_steps:
                alpha = total_steps / fade_in_steps
            else:
                alpha = 1.0 # Stabalisation phase

            # Move real images and labels from the ADNI dataset onto the GPU
            real_images = real_images.to(device)
            class_labels = class_labels.to(device)

            # Determine the batch size
            batch_size = real_images.size(0)

            # Create real, fake and random class labels
            real_labels = torch.full((batch_size,), real_label, dtype=torch.float, device=device)
            fake_labels = torch.full((batch_size,), fake_label, dtype=torch.float, device=device)
            rand_class_labels = torch.randint(0, hp.LABEL_DIMENSIONS, (batch_size,), device=device)

            # ---------------------------------------------------------------------------
            # (1) - Update the Discriminator by training with real images 
            # ---------------------------------------------------------------------------

            # Reset gradients of the discriminator
            disc.zero_grad()

            # Forward pass the real batch through the discriminator
            real_pred, class_real_pred, features_real = disc(real_images, labels=class_labels, depth=current_depth, alpha=alpha)

            # Calculate the real image discriminator losses
            real_loss_disc = adversarial_criterion(real_pred.view(-1), real_labels)
            real_class_loss_disc = class_criterion(class_real_pred, class_labels)

            # Calculate average real prediction value of discriminator
            real_disc_avg = real_pred.mean().item()

            # Calculate the accuracy of the discriminator in classifying classes for real images
            predicted_classes = torch.argmax(class_real_pred, dim=1)
            correct_predictions = (predicted_classes == class_labels).float()
            real_class_disc_acc = correct_predictions.mean().item()

            # ---------------------------------------------------------------------------
            # (2) - Update the Discriminator by training with fake images (by generator)
            # ---------------------------------------------------------------------------

            # Generate a batch of latent vectors to input into the generator
            latent1 = torch.randn(batch_size, hp.LATENT_SIZE, device=device)

            # Create a second latent space batch randomly, for mixing regularisation
            if random.random() < hp.MIXING_PROB:
                latent2 = torch.randn(batch_size, hp.LATENT_SIZE, device=device)
            else:
                latent2 = None

            # Generate fake images using the generator and mixing regularisation
            fake_images, _ = gen(latent1, latent2, mixing_ratio=random.uniform(0.5, 1.0), labels=rand_class_labels, depth=current_depth, alpha=alpha)

            # Apply the discriminator to classify fake images
            fake_pred, class_fake_pred, features_fake = disc(fake_images.detach(), labels=rand_class_labels, depth=current_depth, alpha=alpha)

            # Calculate the fake image discriminator losses
            fake_loss_disc = adversarial_criterion(fake_pred.view(-1), fake_labels)
            fake_class_loss_disc = class_criterion(class_fake_pred, rand_class_labels)

            # Calculate average fake prediction value of discriminator
            fake_disc_avg = fake_pred.mean().item()

            # Calculate gradient penalty
            #grad_penalty = compute_gradient_penalty(disc, real_images, fake_images, current_depth, device)

            # Calculate the total error of the discriminator
            tot_loss_disc = real_loss_disc + real_class_loss_disc + fake_loss_disc + fake_class_loss_disc

            # Update the discriminator based on gradients and losses
            tot_loss_disc.backward()
            grad_norm_disc = torch.nn.utils.clip_grad_norm_(disc.parameters(), max_norm=hp.DISC_GRAD_CLIP)
            disc_opt.step()

            # ---------------------------------------------------------------------------
            # (3) - Update the Generator with new fake images/labels
            # ---------------------------------------------------------------------------

            # Train twice as many times as the Discriminator
            for _ in range(2):
                # Reset gradients of the generator
                gen.zero_grad()

                # Generate a batch of latent vectors to input into the generator
                latent1 = torch.randn(batch_size, hp.LATENT_SIZE, device=device)

                # Create a second latent space batch randomly, for mixing regularisation
                if random.random() < hp.MIXING_PROB:
                    latent2 = torch.randn(batch_size, hp.LATENT_SIZE, device=device)
                else:
                    latent2 = None

                # Generate fake images and labels
                rand_class_labels = torch.randint(0, hp.LABEL_DIMENSIONS, (batch_size,), device=device)
                fake_images, _ = gen(latent1, latent2, mixing_ratio=random.uniform(0.5, 1.0), labels=rand_class_labels, depth=current_depth, alpha=alpha)

                # Forward pass fake images through discriminator (with updated discriminator)
                fake_pred, class_fake_pred, features_fake = disc(fake_images, labels=rand_class_labels, depth=current_depth, alpha=alpha)

                # Calculate the losses of the generator
                loss_gen = adversarial_criterion(fake_pred.view(-1), real_labels)
                loss_class_gen = class_criterion(class_fake_pred, rand_class_labels)
                l2_reg = l2_regularisation(gen)

                # Calculate the total loss of the generator
                tot_loss_gen = loss_gen + loss_class_gen + l2_reg

                # Calculate average output value of discriminator due to generator "real" images
                gen_avg = fake_pred.mean().item()

                # Update the generator based on gradients and losses
                tot_loss_gen.backward()
                grad_norm_gen = torch.nn.utils.clip_grad_norm_(gen.parameters(), max_norm=hp.GEN_GRAD_CLIP)
                gen_opt.step()

            # ---------------------------------------------------------------------------
            # (4) - Logging and Statistics Output 
            # ---------------------------------------------------------------------------

            # Print statistics every 100 steps
            if total_steps % 100 == 0:
                print(f"[Resolution: {current_resolution}x{current_resolution}][Epoch: {epoch + 1}/{hp.EPOCHS_PER_RESOLUTION[current_depth]}][Step: {total_steps}/{total_steps_per_resolution}]\t"
                      f"Loss_D: {tot_loss_disc.item():.4f} | Loss_G: {tot_loss_gen.item():.4f} | Real Pred: {real_disc_avg:.4f} | Real Class Acc: {real_class_disc_acc:.4f} | Alpha: {alpha:.4f} "
                      f"| Disc Gradient: {grad_norm_disc:.4f} | Gen Gradient: {grad_norm_gen:.4f} | Disc Learning Rate {disc_scheduler.get_last_lr()[0]:.6f} | Gen Learning Rate {gen_scheduler.get_last_lr()[0]:.6f}")

            # Save Losses for plotting later
            G_losses.append(tot_loss_gen.item())
            D_losses.append(tot_loss_disc.item())

            # Regularly save an image out of the generator to see progress overtime 
            if (iters % 500 == 0) or (total_steps == total_steps_per_resolution- 1):
                with torch.no_grad():
                    fake, _ = gen(fixed_noise, depth=current_depth, alpha=alpha)
                    fake = fake.detach().cpu()
                img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

            iters += 1
            total_steps += 1

            # Step the schedulers at the end of each step/batch
            gen_scheduler.step()
            disc_scheduler.step()

        # Plot after each epoch to visualise training
        with torch.no_grad():
            fake, _ = gen(fixed_noise, depth=current_depth, alpha=alpha)
            fake = fake.detach().cpu()
        plt.figure(figsize=(8,8))
        plt.axis("off")
        plt.title(f"Snippet of Image Generation at Resolution {current_resolution}x{current_resolution}")
        plt.imshow(np.transpose(vutils.make_grid(fake[:64], padding=2, normalize=True).cpu(),(1,2,0)))
        plt.savefig(os.path.join(hp.SAVED_OUTPUT_DIR, "during_training_images.png"), pad_inches=0)
        plt.close()

        # Epoch training time
        epoch_duration = ((time.time() - epoch_start_time) / 60) # In minutes
        print(f"Epoch [{epoch + 1}/{hp.EPOCHS_PER_RESOLUTION[current_depth]}] completed in {epoch_duration:.2f} minutes")

    # End of a resolution stage so save the images, loss plot and models
    save_resolution(disc, gen, D_losses, G_losses, current_resolution, current_depth, device)

# Total training time 
total_training_time = ((time.time() - training_start_time) / 60) # In minutes
print(f"Total training time: {total_training_time:.2f} minutes")