# StyleGAN2 Based Image Generation Network on ANDI Brain Dataset
## Project Aim
The aim of the model is to create a Image generation network of the ADNI brain dataset based on StyleGAN2, and to 
investigate the relationship between the style latent space of models on different classes of the dataset.

## Dependencies

    |  Libaries       |Version            |
    |-----------------|-------------------|
    |  python         |3.11.9             |
    |  torch          |2.5.0              |
    |  torchvision    |0.20.0             |
    |  pytorch-cuda   |11.8               |
    |  cudatoolkit    |10.1               |
    |  numpy          |1.23.5             |
    |  matplotlib     |3.9.2              |
    |  tqdm           |4.66.5             |
    |  umap-learn     |0.5.4              |
    |  pyqt           |5.15.10            |


## Dataset

This project trains on the ADNI dataset. The ADNI (Alzheimer's Disease Neuroimaging Initiative) dataset contains two 
classes: Alzheimer’s disease (AD) and Normal Cognition (NC). 

## StyleGAN2 architecture

This project implements StyleGAN2. StyleGAN2 is an improvement from StyleGAN, which is based off of Generative Adversarial Networks (GANs). A GAN is composed 
of two main components: a generator and discriminator. Generator will generate fake images given an input noise, and the
discriminator is a classifier that classifies whether the given input image is real or fake.

StyleGAN improves GAN by the following:
- Instead of a traditional latent input, generator takes a learned constant. 
- Using a standalone mapping network and AdaIN for style transfer.
  - The mapping network is a neural network composed of 8 fully connected layers. It takes a randomly sampled point from
  the latent space and generates a style vector. The style vector is then transformed and incorporated to each block of
  generator via adaptive instance normalization (AdaIN). 
- Addition of noise to each layer.
  - The noise is used to generate style-level variation.
- Mixing regularization
  - Since the Style generation uses intermediate vector at each level of synthesis, the network may learn the 
  correlation between the levels. Mixing regularization is introduced to reduce the correlation between levels.
![stylegan.png](stylegan.png)

StyleGAN2 improves StyleGAN by addressing the shortcoming of StyleGAN, namely, the droplet effect and the preference of 
features over a certain position. Weight modulation and demodulation is used, and got rid of progressive growing
![stylegan2.png](stylegan2.png)
## Algorithm
The architecture have 3 main components: Discriminator, Generator, and Noise Mapping Network.
These three components are defined in module.py as classes. 

The Generator is made up of multiple generator blocks and a 
StyleBlock. The GeneratorBlock is made up of two connected StyleBlock and followed by a ToRGB block.

The Discriminator is a simple classification neural network with Adam optimizer.


Adam was used as the optimizer for generator, discriminator and the mapping network.
Hyperparameter used include batch_size = 32, learning rate = 1e-3, LAMBDA_GP (for Adam optimizer) = 10. The model 
generates images with resolution 256x256. This is chosen as it is closest to the original image resolution, which is 
240x256.

## Preprocessing
The images are cropped to 256x256, and then random horizontal flip was applied.

## Usage
Before training or generating examples using trained model, please download the ADNC dataset. Replace all instances of 
{AD_NC} below with the directory of your downloaded dataset.

To train on the entire dataset (without separating AD and NC classes, run:

```python3 train.py --dataset_dir {directory} --classes all```

To train specifically only on AD or NC brain images, run:

```python3 train.py --dataset_dir {directory} --classes AD``` or

```python3 train.py --dataset_dir {directory} --classes NC```

Models are saved to model folder under current directory by default. If you wish to save model and load model from a
different directory, add your directory with 

```--model_dir {directory}```

example usage:

`python3 train.py --model_dir model --classes AD --dataset_dir AD_NC/train`

To see examples using a trained model, run 

```python3 predict.py --dataset_dir AD_NC/train --model_dir model --plot_umap True --AD_dir AD --NC_dir NC```

If plot_umap is set to be True, --AD_dir and --NC_dir must be specified.

## Training result
### Comparison of fake images and real images
![epoch_0.png](fake_vs_real%2Fepoch_0.png)
![epoch_20.png](fake_vs_real%2Fepoch_20.png)
![epoch_30.png](fake_vs_real%2Fepoch_30.png)

### training loss
In total, 3 models were trained. A general model that trained on both AD and NC, an AD-only model, and a NC-only model.

#### Loss of general model
![training_loss.png](training%2Ftraining_loss.png)
![training_loss_proportion.png](training%2Ftraining_loss_proportion.png)

#### Loss of AD model
![training_loss_AD.png](training%2Ftraining_loss_AD.png)
![training_loss_proportion_AD.png](training%2Ftraining_loss_proportion_AD.png)

#### Loss of NC model
![training_loss_NC.png](training%2Ftraining_loss_NC.png)
![training_loss_proportion_NC.png](training%2Ftraining_loss_proportion_NC.png)

### UMAP embedding
Aside from a model that trained on both AD and NC brain images indiscriminately, two additional models were trained on 
only AD brain images and only NC brain images. The style embedding output by the mapping network was plotted using umap.
10001 random noise was put in to the map to get the output with 20002 points, where there are 10001 points of AD mapping
and 10001 of NC mapping.

When the model was initialized, there is no distinction between umap embeddings of AD and NC. This is expected as the 
mapping network is untrained.

As the training progresses, the embedding starts to moving away from each other. At around epoch 30, it is observed that
the two classes overlap, but are separate. This agrees with the recent UQ work (Liu, S. et al., 2023), suggesting that 
AD could be a progressive disease.


![umap_epoch_0.png](umap%20plot%2Fumap_epoch_0.png)
![umap_epoch_5.png](umap%20plot%2Fumap_epoch_5.png)
![umap_epoch_30.png](umap%20plot%2Fumap_epoch_30.png)

## Reference 
Ian J. Goodfellow et al. (2014). Generative Adversarial Networks https://arxiv.org/pdf/1406.2661 

Tero Karras et al. (2018). A Style-Based Generator Architecture for Generative Adversarial Networks https://doi.org/10.48550/arXiv.1812.04948

Jason Brownlee (2020). A Gentle Introduction to StyleGAN the Style Generative Adversarial Network https://machinelearningmastery.com/introduction-to-style-generative-adversarial-network-stylegan/

Abd Elilah TAUIL (2023). Understanding StyleGAN2 https://blog.paperspace.com/understanding-stylegan2/

Liu, S. et al. (2023). Style-Based Manifold for Weakly-Supervised Disease Characteristic Discovery. In: Greenspan, H., et al. Medical Image Computing and Computer Assisted Intervention – MICCAI 2023. MICCAI 2023. Lecture Notes in Computer Science, vol 14224. Springer, Cham. https://doi.org/10.1007/978-3-031-43904-9_36