# Image Generation using VQ-VAE and PixelCNN
**Author:** Harshit Rathore, 48143749.
## Overview
This project implements **Vector Quantised-Variational Autoencoder (VQ-VAE)** combined with **PixelCNN** to generate high-quality images. VQ-VAE leverages discrete latent spaces to overcome limitations like posterior collapse found in traditional VAEs. Instead of learning continuous latents, the encoder maps the input data to a discrete codebook, providing compressed but meaningful representations.

Once the VQ-VAE learns the latent space, PixelCNN models the prior distribution over these discrete latent variables. PixelCNN captures the dependencies within these codes, allowing it to generate new samples by sequentially predicting one pixel/code at a time conditioned on previously generated ones. This model combination ensures high-quality generation, with meaningful features learned in the latent space and improved sample diversity through autoregressive priors.

## VQ-VAE: Discrete Latent Space for Efficient Encoding

![VQ-VAE Model](./Images/vqvae.model.png)

### **Concept of VQ-VAE**
VQ-VAE improves on traditional VAEs by using **discrete latent variables**. The encoder maps input images to continuous latent vectors, which are then **quantized to discrete codes** by selecting the nearest entry from a **learned codebook**. This quantization ensures that the model focuses on learning useful features rather than trivial or noisy details.

### **Workflow of VQ-VAE**
1. **Encoder**: Maps input image $x$ to a continuous latent vector $z_e(x)$.

$$z_e(x) = \text{Encoder}(x)$$

2. **Quantization**: Finds the nearest embedding vector $e_k$ from the codebook for each latent vector.

$$z_q(x) = e_k \quad \text{where} \quad k = \arg \min_j \|z_e(x) - e_j\|^2$$

3. **Decoder**: Uses the quantized latent vector $z_q(x)$ to reconstruct the original image.

$$\hat{x} = \text{Decoder}(z_q(x))$$

4. **Training Objective**: VQ-VAE minimizes a combination of three losses:
   - **Reconstruction Loss**: Measures the difference between the input and reconstructed image.

$$L_{\text{recon}} = \|x - \hat{x}\|_2^2$$

   - **Codebook Loss**: Moves the codebook embeddings closer to the encoder output.

$$L_{\text{codebook}} = \| \text{sg}(z_e(x)) - e_k \|_2^2$$
   - **Commitment Loss**: Encourages the encoder to commit to a specific code in the codebook.

$$L_{\text{commit}} = \beta \| z_e(x) - \text{sg}(e_k) \|_2^2$$

   **Total Loss**:

$$L = L_{\text{recon}} + L_{\text{codebook}} + L_{\text{commit}}$$

   Here, **sg** denotes the **stop-gradient** operation, preventing the gradients from flowing into the codebook embeddings.



## PixelCNN: Autoregressive Prior over Latent Codes

### **Concept of PixelCNN**
PixelCNN is an **autoregressive model** designed to learn the dependencies between pixels (or, in this case, latent codes). In this project, **PixelCNN models the prior distribution over the discrete latent codes generated by VQ-VAE**, allowing it to sample new latent codes that can be decoded into novel images.

### **How PixelCNN Works for VQ-VAE**
- **Training Phase**: PixelCNN learns the joint distribution of the discrete latent codes produced by the VQ-VAE encoder. Each code is predicted sequentially, conditioned on previously generated codes.
  
- **Generation Phase**: PixelCNN generates **quantized latent codes**, which are then decoded by the VQ-VAE decoder to generate a new image. This ensures that the sampled images follow the structure and style of the training data.

### **Mathematical Formulation**
The joint distribution over latent codes is factorized as:

$$p(z) = \prod_{i=1}^{n} p(z_i | z_1, \ldots, z_{i-1})$$

Where $z_i$ is the code for the $i^{th}$ position in the latent space. 

During training, PixelCNN maximizes the **log-likelihood** of the latent codes:

$$L_{\text{PixelCNN}} = - \sum_{i=1}^{n} \log p(z_i | z_1, \ldots, z_{i-1})$$

This formulation ensures that the model learns the dependencies between latent codes, making it capable of generating realistic samples when used in combination with the VQ-VAE decoder.


## How VQ-VAE and PixelCNN Work Together for Image Generation

1. **Training VQ-VAE**:
   - The VQ-VAE encoder learns to map input images to discrete latent codes.
   - The VQ-VAE decoder reconstructs the input images from these codes.

2. **Training PixelCNN**:
   - PixelCNN is trained on the quantized latent space generated by the VQ-VAE encoder.
   - It learns to model the dependencies between the latent codes, acting as a **prior distribution**.

3. **Generating New Images**:
   - PixelCNN samples a sequence of latent codes following the learned distribution.
   - These sampled codes are passed to the VQ-VAE decoder to generate a new image.

This two-stage pipeline ensures high-quality and realistic image generation, with PixelCNN providing structure to the discrete latent space learned by VQ-VAE.


## Dataset Details

The dataset used in this project contains 2D Hip MRI **12,460 images**:
- **12,400 images** are of size **256 × 128 pixels**.
- **60 images** are of size **256 × 144 pixels**. These were **filtered out (removed)** to maintain consistent dimensions during training.

![GenSample Dataset Images](./Images/sample_visualization.png)



## Project Structure
[`dataset.py`](./dataset.py): Contains the dataset loader, which handles data pre-processing and filtering of inconsistent image dimensions.

[`modules.py`](./modules.py): Implements both the **VQ-VAE** and **PixelCNN** models.

[`train_VQVAE.py`](./train_VQVAE.py): Trains the **VQ-VAE** model to learn the latent representations.

[`train_PixelCNN.py`](./train_PixelCNN.py): Trains the **PixelCNN** to model the prior distribution over the **VQ-VAE**’s discrete latent space.

[`predict.py`](./predict.py): Compares original images with their reconstructed versions and generates new images using both the **VQ-VAE** and **PixelCNN** models.

[`visualization_utils.py`](./visualization_utils.py): Provides utilities to plot loss curves, generate image comparisons, and visualize training progress.

[`environment.yml`](./environment.yml): Contains a complete list of dependencies to **reproduce the environment** used in this project.

## How to Use

### 1. Setting Up the Environment
To reproduce the environment, use the provided `environment.yml` file:

```bash
conda env create -f environment.yml
conda activate gpupytorch
```

The following dependencies are required for this project:

| **Dependency**  | **Version**|
|-----------------|----------|
| PyTorch         | 2.4.0    |
| TQDM            | 4.66.5   |
| NumPy           | 1.26.4   |
| Matplotlib      | 3.8.4    |
| Pillow (PIL)    | 10.4.0   |
| scikit-image    | 0.23.2   |
| NiBabel         | 5.2.1    |

Requirements.txt file is provided to install these libraries.

### 2. Training VQ-VAE
```bash
python train_VQVAE.py
```

### 3. Training PixelCNN
```bash
python train_PixelCNN.py
```
### 4. Generating Samples and Comparing Reconstructions
```bash
python predict.py
```

## Training and Loss Curves

### Hyperparameters for VQ-VAE and PixelCNN Models

| Hyperparameter       | Value  | Description                                                                                                                                            |
|----------------------|--------|--------------------------------------------------------------------------------------------------------------------------------------------------------|
| **BATCH_SIZE**       | 32     | Number of training samples processed in one forward/backward pass. Affects computation efficiency and frequency of model weight updates.              |
| **HIDDEN_DIM**       | 128    | Number of hidden units in the model's latent representation. Higher values enable more complex representations.                                        |
| **NUM_RESIDUAL_LAYER** | 2   | Number of residual layers in the model. Residual layers improve training stability and help capture nuanced data features.                            |
| **RESIDUAL_HIDDEN_DIM** | 32 | Dimensionality of hidden units within each residual layer. Balances between model efficiency and performance on complex data.                        |
| **NUM_EMBEDDINGS**   | 128    | Number of embeddings (discrete latent vectors) in the VQ-VAE codebook, allowing for unique quantized representations of the data.                    |
| **EMBEDDING_DIM**    | 128    | Dimensionality of each embedding vector in the codebook, enhancing data encoding richness for better output quality.                                 |
| **COMMITMENT_COST**  | 0.25   | Regularization parameter that penalizes differences between encoder output and nearest embedding vector, encouraging effective embedding usage.       |
| **LEARNING_RATE**    | 1e-2   | Step size for model updates during optimization. Higher values speed up training, while lower rates improve precision.                               |
| **NUM_EPOCH_VQVAE**  | 1000   | Total number of training epochs for VQ-VAE model (full passes through the dataset). Higher values allow extended training, beneficial for larger datasets.             |
| **NUM_EPOCH_PixelCNN**  | 150   | Total number of training epochs for PixelCNN model (full passes through the dataset).             |


### Loss Curve for the VQ-VAE Model

The loss curve is separated in two sections for better visualization. The first loss curve(initial 50 epochs) presents the convergence of the model. The model converged with in 50 epochs, very very slight improvement were seen in the model after that, which is presented in second loss curve. After 50 epoch, the model's loss only improved by 0.003, indicating that the model had reached a plateau in terms of accuracy and loss reduction.

![VQ-VAE 50 Epoch Loss](./Images/loss_curve50.png)

Training Loss Curve of VQ-VAE for first 50 epoch.

![VQ-VAE 50 Beyond Epoch Loss](./Images/loss_curve50beyond.png)

Training Loss Curve for VQ-VAE beyond 50 epoch.

### Loss Curve for the PixelCNN Model

The model successfully converged after 120 epochs, demonstrating stable training and optimal performance. No significant improvements were observed beyond this point, indicating that the model had reached a plateau in terms of accuracy and loss reduction.

![PixelCNN Los](./Images/CNN_loss.png)

Training Loss Curve for PixelCNN.

## Prediction Comparison

### Below is the comparison of the original images and reconstructed images from VQ-VAE Model. On an average these images have the **SSIM score of 0.90**

![Original Images](./Images/original_image.jpg)

These are the batch of Original Images passed to the VQ-VAE Model.

![Generated Images](./Images/generated_image.jpg)

These are the batch of Generated Images passed to the VQ-VAE Model.

List of SSIM score for this batch:

SSIM Score for image 0: 0.8926769512723215\
SSIM Score for image 1: 0.8795653320531376\
SSIM Score for image 2: 0.9128573662257585\
SSIM Score for image 3: 0.933126552460135\
SSIM Score for image 4: 0.9201359825896435\
SSIM Score for image 5: 0.836747427980675\
SSIM Score for image 6: 0.9237493664158172\
SSIM Score for image 7: 0.8979885293753421\
SSIM Score for image 8: 0.8971284876275258\
SSIM Score for image 9: 0.9257338061315603\
SSIM Score for image 10: 0.8809121275690736\
SSIM Score for image 11: 0.8980848514774784\
SSIM Score for image 12: 0.8787912491735865\
SSIM Score for image 13: 0.9046357731275627\
SSIM Score for image 14: 0.9001704167660143\
SSIM Score for image 15: 0.9013343843847025\
SSIM Score for image 16: 0.9172732051063757\
SSIM Score for image 17: 0.9257412775127125\
SSIM Score for image 18: 0.9061774743932193\
SSIM Score for image 19: 0.9047303822497517\
SSIM Score for image 20: 0.8832726799425532\
SSIM Score for image 21: 0.8771655908608046\
SSIM Score for image 22: 0.8995690258506869\
SSIM Score for image 23: 0.9058141686237249\
SSIM Score for image 24: 0.9233696606931139\
SSIM Score for image 25: 0.9356542708209303\
SSIM Score for image 26: 0.9033189999390637\
SSIM Score for image 27: 0.9217855303746755\
SSIM Score for image 28: 0.9097960419058799\
SSIM Score for image 29: 0.912894228213146\
SSIM Score for image 30: 0.9077796540651165\
SSIM Score for image 31: 0.9303947841412709

### These are the Latent Spaces and Quantized spaces from the model.


![Single Channel of Latent Space](./Images/latent_space.png)

Single Channel of Latent Space from VQ-VAE Model.


![Single Channel of Quantised Space](./Images/quantized_space.png)

Single Channel of Quantised Latent Space from VQ-VAE Model.

### Image Generated using PixelCNN and VQ-VAE.

![PixelCNN Generated Images](./Images/reconstructed_image.png)

## Potential Improvements

1. **Improved Quantization:** Use dynamic or hierarchical codebooks for more adaptive and complex representations.

2. **PixelCNN Variants:** Explore advanced models like PixelSNAIL or Gated PixelCNN for better image sharpness and diversity.

3. **Latent Space Regularization:** Apply techniques like orthogonalization or sparsity regularization to encourage a more structured latent space.

4. **Hybrid Models (VQ-VAE + GANs):** Combine with GANs to improve image realism and detail through adversarial loss.

5. **Broader Evaluation Metrics:** Use additional metrics like FID, PSNR, or Inception Score for a more comprehensive image quality assessment.

## References

* [1] VQ-VAEs: Neural Discrete Representation Learning: https://www.youtube.com/watch?v=VZFVUrYcig0.
* [2] Paper: *Neural Discrete Representation Learning*, Aaron van den Oord, Oriol Vinyals, Koray Kavukcuoglu, 2017. https://arxiv.org/abs/1711.00937
* [3] Sonnet VQ-VAE implementation: https://github.com/google-deepmind/sonnet/blob/v1/sonnet/examples/vqvae_example.ipynb.
* [4] PixelCNN Theory: https://sergeiturukin.com/2017/02/22/pixelcnn.html 
* [5] Albert's Repo for code reference: https://github.com/Albert-bc/vq-vae/tree/topic-recognition